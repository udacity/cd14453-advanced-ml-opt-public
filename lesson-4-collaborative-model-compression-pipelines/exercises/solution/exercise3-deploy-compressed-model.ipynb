{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Deploy a compressed model to mobile with ExecuTorch and ONNX\n",
    "\n",
    "In this exercise, you'll prepare and deploy a compressed model using two popular mobile deployment frameworks: PyTorch's Executorch and ONNX Runtime. You'll compare their performance characteristics to make an informed deployment decision.\n",
    "\n",
    "> **Task**: Convert a compressed language model to both Executorch and ONNX formats, and compare their performance for mobile deployment.\n",
    "> \n",
    "> **Goal**: By the end of this exercise, you'll understand the tradeoffs between different mobile deployment frameworks and learn how to optimize models for specific target platforms.\n",
    "> \n",
    "> **Scenario**:  Your team has developed a image classification model for that can run directly on mobile devices to protect customer privacy and ensure responsiveness even with poor connectivity. You've been tasked with preparing the compressed model for deployment to various Android and iOS devices.\n",
    "> \n",
    "> **Tools**: pytorch, executorch, onnx, onnxruntime, matplotlib\n",
    "> <br> _Prior experience recommended!_\n",
    "> \n",
    "> **Estimated Time**: 15 minutes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "First, let's set up our environment with the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out the cell block again and restart the notebook\n",
    "# ! pip install --upgrade torch torchvision executorch onnx onnxruntime flatbuffers typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "PyTorch version: 2.7.0+cu126\n",
      "Torchvision version: 0.22.0+cu126\n",
      "ONNX Runtime version: 1.22.0\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "from torch.export import Dim, export\n",
    "from executorch.exir import to_edge_transform_and_lower\n",
    "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
    "from executorch.runtime import Runtime\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# TODO: Set up the device to CPU for simulating mobile conditions\n",
    "# Hint: Use torch.device with the appropriate string argument\n",
    "device = torch.device(\"cpu\")  # Add your code here\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"assets/exercise3\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure libraries are in path\n",
    "os.environ[\"PATH\"] = f\"{os.path.expanduser('~/.local/bin')}:\" + os.environ[\"PATH\"]\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"ONNX Runtime version: {ort.__version__}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why are we using CPU instead of GPU?**\n",
    "> We are deliberately using CPU for benchmarking to simulate our mobile device conditions. By using CPU, we're creating a more realistic test environment that aligns with the constraints you'll face in the real-world deployment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load the pre-compressed model\n",
    "\n",
    "We'll use a pre-trained and compressed MobileNetV3-Small model, which is already optimized for mobile deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /home/student/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.83M/9.83M [00:00<00:00, 80.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MobileNetV3-Small\n",
      "Number of parameters: 2,542,856\n",
      "Model size: 9.75 MB\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "weights = MobileNet_V3_Small_Weights.DEFAULT\n",
    "model = mobilenet_v3_small(weights=weights)\n",
    "\n",
    "# TODO: Move the model to the device \n",
    "# Hint: Use one of the PyTorch model's built-in methods with the `device` variable set up in Step 1.\n",
    "# See: https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "model = model.to(device)  # Add your code here\n",
    "\n",
    "# TODO: Set the model to evaluation mode\n",
    "# Hint: PyTorch models have an evaluation mode for inference\n",
    "# See: https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "model.eval()  # Add your code here\n",
    "\n",
    "# Calculate model size and parameters\n",
    "param_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
    "buffer_size = sum(b.numel() * b.element_size() for b in model.buffers()) / (1024 * 1024)\n",
    "model_size = param_size + buffer_size\n",
    "\n",
    "print(f\"Model: MobileNetV3-Small\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model size: {model_size:.2f} MB\")\n",
    "\n",
    "# Load ImageNet class labels\n",
    "with open(output_dir / \"imagenet_classes.json\", \"w\") as f:\n",
    "    class_idx = json.dumps({str(i): weights.meta[\"categories\"][i] for i in range(1000)})\n",
    "    f.write(class_idx)\n",
    "\n",
    "# Define image preprocessing pipeline\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why MobileNetV3-Small?** MobileNetV3 represents a family of models specifically designed for mobile deployment. These models use several compression techniques:\n",
    "> \n",
    "> - *Depthwise Separable Convolutions* - Factorizes standard convolutions into depthwise and pointwise operations, reducing computation by 8-9x\n",
    "> - *Squeeze-and-Excitation* - Adds channel attention mechanisms with minimal overhead\n",
    "> - *Architectural Search* - The model architecture was optimized using Neural Architecture Search (NAS)\n",
    "> - *Activation Functions* - Uses h-swish activation, which is a hardware-friendly approximation of swish\n",
    "> \n",
    "> These built-in optimizations make it an excellent starting point for mobile deployment, requiring less manual compression compared to standard architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define sample inputs for testing\n",
    "\n",
    "Let's load and prepare a sample image for testing our model conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input shape: torch.Size([1, 3, 224, 224])\n",
      "Original model output shape: torch.Size([1, 1000])\n",
      "Top prediction: 21\n"
     ]
    }
   ],
   "source": [
    "def prepare_sample_image(size=224, batch_size=1):\n",
    "    \"\"\"Create sample image data for inference testing.\"\"\"\n",
    "    # Create a random RGB image tensor\n",
    "    image = torch.rand(batch_size, 3, size, size, device=device)\n",
    "    return image\n",
    "\n",
    "# Create sample input\n",
    "sample_input = prepare_sample_image(size=224, batch_size=1)\n",
    "print(f\"Sample input shape: {sample_input.shape}\")\n",
    "\n",
    "# TODO: Move the input tensor to CPU for compatibility with deployment frameworks\n",
    "# Hint: PyTorch tensors have a built-in method for this functionality\n",
    "# See: https://docs.pytorch.org/docs/stable/tensors.html \n",
    "sample_input = sample_input.cpu()  # Add your code here\n",
    "\n",
    "# Test the original model with the sample input\n",
    "with torch.no_grad():\n",
    "    original_output = model(sample_input)\n",
    "    \n",
    "print(f\"Original model output shape: {original_output.shape}\")\n",
    "print(f\"Top prediction: {original_output.argmax(dim=1).item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Should we use random data?** In the real world, the answer is better not to! While synthetic data works well for pure performance benchmarking, you would also test with representative real-world data to validate model accuracy. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Export model to ONNX format\n",
    "Now, let's export the PyTorch model to ONNX format. \n",
    "\n",
    "In our example, we have assumed the model to already have been compressed. In practice, you'd typically run post-training quantization (to INT8!) and/or graph optimizations as part of the export.\n",
    "\n",
    "**Note**: We don't need to export to a different format for ExecuTorch which, unlike the previous PyTorch Mobile, does not require conversion to TorchScript!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model saved to assets/exercise3/models/mobilenetv3_small.onnx\n",
      "ONNX model size: 9.71 MB\n"
     ]
    }
   ],
   "source": [
    "def export_to_onnx(model, sample_input, onnx_path):\n",
    "    \"\"\"Export model to ONNX format.\"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(onnx_path), exist_ok=True)\n",
    "    \n",
    "    # TODO: Step 1. Define dynamic axes for batch and input dimensions\n",
    "    # Hint: The dictionary should define which dimensions can vary (like batch size)\n",
    "    # See: https://pytorch.org/docs/stable/onnx.html#torch.onnx.export\n",
    "    dynamic_axes = {\n",
    "        'input': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }  # Add your code here \n",
    "    \n",
    "    # Step 2. Export the model to ONNX format\n",
    "    with torch.no_grad():\n",
    "        torch.onnx.export(\n",
    "            model,               # model being run\n",
    "            sample_input,        # model input (or a tuple for multiple inputs)\n",
    "            onnx_path,           # where to save the model\n",
    "            export_params=True,  # store the trained parameter weights inside the model file\n",
    "            opset_version=12,    # the ONNX version to export the model to\n",
    "            input_names=['input'],     # the model's input names\n",
    "            output_names=['output'],   # the model's output names\n",
    "            dynamic_axes=dynamic_axes   # variable length axes\n",
    "        )\n",
    "    \n",
    "    # TODO: Step 3: Verify the model structure and check for errors\n",
    "    # Hint: You only need one single call to a built-in onnx method!\n",
    "    # See: https://onnx.ai/onnx/api/checker.html\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(onnx_model)  # Add your code here \n",
    "    \n",
    "    print(f\"ONNX model saved to {onnx_path}\")\n",
    "    print(f\"ONNX model size: {os.path.getsize(onnx_path) / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    return onnx_path\n",
    "\n",
    "# Export the model to ONNX\n",
    "onnx_path = str(output_dir / \"models/mobilenetv3_small.onnx\")\n",
    "exported_onnx_path = export_to_onnx(model, sample_input, onnx_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Understanding ONNX export parameters**: Export parameters are crucial for handling the real-world conditions you expect for your application once deployed.\n",
    "> \n",
    "> - *dynamic_axes*: Allows the model to handle varying input dimensions (like different batch sizes)\n",
    "> - *opset_version*: Defines which ONNX operations are available (higher versions support more operations)\n",
    "> - *do_constant_folding*: Pre-computes constant expressions to optimize inference\n",
    "> - *input_names/output_names*: Names tensors for easier integration with inference engines\n",
    "> \n",
    "> _**IMPORTANT**_: Operators not supported by your chosen opset version can cause export failures, a common issue when deploying to mobile."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare model for mobile deployment\n",
    "\n",
    "Let's optimize both models specifically for mobile deployment. \n",
    "\n",
    "Mobile devices have unique constraints like limited battery life, restricted memory, and diverse hardware capabilities. Frameworks can address these specific constraints out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model to Executorch...\n",
      "Executorch model saved to assets/exercise3/models/mobilenetv3_small.pte\n",
      "Executorch model size: 9.76 MB\n"
     ]
    }
   ],
   "source": [
    "def export_to_executorch(model, sample_input, output_path):\n",
    "    \"\"\"Convert PyTorch model to Executorch format.\"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    print(\"Exporting model to Executorch...\")\n",
    "\n",
    "    # TODO: Step 1: Define dynamic shapes for varying input sizes\n",
    "    # Hint: Use Dim objects to define min/max ranges for each dimension\n",
    "    # See: https://pytorch.org/docs/stable/export.html    \n",
    "    dynamic_shapes = {\n",
    "        # For MobileNetV3, let's assume height and width can vary between 224 and 640\n",
    "        \"x\": {\n",
    "            2: Dim(\"h\", min=224, max=640),\n",
    "            3: Dim(\"w\", min=224, max=640),\n",
    "        }\n",
    "    }  # Add your code here \n",
    "    \n",
    "    # Step 2: Export the model using torch.export\n",
    "    exported_program = export(model, (sample_input,), dynamic_shapes=dynamic_shapes)\n",
    "    \n",
    "    # TODO: Step 3: Transform to mobile and lower with XNNPACK partitioner\n",
    "    # Hint: The required methods have already been imported in Step 1.!\n",
    "    # See: https://docs.pytorch.org/executorch/stable/export-to-executorch-api-reference.html\n",
    "    executorch_program = to_edge_transform_and_lower(\n",
    "        exported_program,\n",
    "        partitioner=[XnnpackPartitioner()]\n",
    "    ).to_executorch()\n",
    "    \n",
    "    # Step 4: Save the program to a file\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(executorch_program.buffer)\n",
    "        \n",
    "    print(f\"Executorch model saved to {output_path}\")\n",
    "    print(f\"Executorch model size: {os.path.getsize(output_path) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# Export the model to ExecuTorch for mobile deployment\n",
    "executorch_path = str(output_dir / \"models/mobilenetv3_small.pte\")\n",
    "export_to_executorch(\n",
    "    model, sample_input, executorch_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized ONNX model saved to assets/exercise3/models/mobilenetv3_small_optimized.onnx\n",
      "Optimized ONNX model size: 9.71 MB\n"
     ]
    }
   ],
   "source": [
    "def optimize_onnx_for_mobile(onnx_path):\n",
    "    \"\"\"Optimize ONNX model for mobile deployment.\"\"\"\n",
    "    # Load the ONNX model\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    \n",
    "    # Create optimized model path\n",
    "    optimized_path = onnx_path.replace(\".onnx\", \"_optimized.onnx\")\n",
    "    \n",
    "    # TODO: Configure session options for mobile-specific optimizations\n",
    "    # Hint: You should set `.graph_optimization_level` at minimum. Also, how do you save the converted model?\n",
    "    # See: https://onnxruntime.ai/docs/api/python/api_summary.html#sessionoptions\n",
    "    sess_options = ort.SessionOptions()\n",
    "    # Add your code here\n",
    "    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "    sess_options.optimized_model_filepath = optimized_path\n",
    "    \n",
    "    # Create a session with the options (this will save the optimized model)\n",
    "    _ = ort.InferenceSession(onnx_path, sess_options)\n",
    "    \n",
    "    print(f\"Optimized ONNX model saved to {optimized_path}\")\n",
    "    print(f\"Optimized ONNX model size: {os.path.getsize(optimized_path) / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    return optimized_path\n",
    "\n",
    "# Optimize the ONNX model for mobile\n",
    "optimized_onnx_path = optimize_onnx_for_mobile(exported_onnx_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Benchmark the model with both frameworks\n",
    "\n",
    "Now, let's compare the performance of the models with both frameworks.\n",
    "\n",
    "**IMPORTANT**: While mathematically equivalent, the reorderings applied by model conversion to ONNX and ExecuTorch can cause small numerical differences. As long as these differences are extremely small (in the 10^-5 to 10^-6 range), this level of variation can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[program.cpp:135] InternalConsistency verification requested but not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking ExecuTorch model...\n",
      "Loading Executorch model from assets/exercise3/models/mobilenetv3_small.pte...\n",
      "Running warmup...\n",
      "Running benchmark...\n",
      "Benchmarking ONNX Runtime model...\n",
      "\n",
      "Benchmark Results:\n",
      "              accuracy_check  accuracy_diff  avg_latency_ms  p90_latency_ms  \\\n",
      "framework                                                                     \n",
      "Executorch              True       0.000014     2239.517164     4557.667303   \n",
      "ONNX Runtime            True       0.000007      121.779823      162.108755   \n",
      "\n",
      "              max_latency_ms  min_latency_ms  throughput  model_size_mb  \n",
      "framework                                                                \n",
      "Executorch       5396.149397      701.857805    0.446519       9.758087  \n",
      "ONNX Runtime      199.659586       99.669218    8.211110       9.712790  \n"
     ]
    }
   ],
   "source": [
    "def benchmark_executorch(model_path, sample_input, n_runs=5):\n",
    "    \"\"\"Benchmark Executorch model performance.\"\"\"\n",
    "    \n",
    "    # TODO: Load and initialize the Executorch model\n",
    "    # Hint: Think of Executorch like a minimal runtime engine for mobile inference.\n",
    "    # See: https://docs.pytorch.org/executorch/stable/getting-started.html#testing-the-model\n",
    "    print(f\"Loading Executorch model from {model_path}...\")\n",
    "    runtime = Runtime.get()  # Add your code here \n",
    "    program = runtime.load_program(model_path)  # Add your code here \n",
    "    method = program.load_method(\"forward\")  # Add your code here \n",
    "    \n",
    "    # Warmup\n",
    "    print(\"Running warmup...\")\n",
    "    for _ in range(10):\n",
    "        _ = method.execute([sample_input])\n",
    "    \n",
    "    # Benchmark\n",
    "    print(\"Running benchmark...\")\n",
    "    latencies = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_runs):\n",
    "            start = time.time()\n",
    "            ts_output = method.execute([sample_input])\n",
    "            latencies.append((time.time() - start) * 1000)  # ms\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Compare outputs\n",
    "    is_output_close = torch.allclose(original_output, ts_output[0], rtol=1e-3, atol=1e-5)\n",
    "    max_accuracy_diff = (original_output - ts_output[0]).abs().max().item()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"framework\": \"Executorch\",\n",
    "        \"accuracy_check\": is_output_close,\n",
    "        \"accuracy_diff\": max_accuracy_diff,\n",
    "        \"avg_latency_ms\": np.mean(latencies),\n",
    "        \"p90_latency_ms\": np.percentile(latencies, 90),\n",
    "        \"max_latency_ms\": np.max(latencies),\n",
    "        \"min_latency_ms\": np.min(latencies),\n",
    "        \"throughput\": n_runs / total_time,\n",
    "        \"model_size_mb\": os.path.getsize(model_path) / (1024 * 1024)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def benchmark_onnx(model_path, sample_input, n_runs=5):\n",
    "    \"\"\"Benchmark ONNX model performance.\"\"\"\n",
    "    \n",
    "    # TODO: Define the execution configuration for ONNX to simulate a mobile environment\n",
    "    # Hint: \n",
    "    # - How many CPU threads are typically available for an app?\n",
    "    # - Would you want to use GPU acceleration here?\n",
    "    # - What kind of optimization level balances speed and compatibility?\n",
    "    # - How should execution behave on low-core devices?\n",
    "    # See: https://onnxruntime.ai/docs/performance/tune-performance/threading.html\n",
    "    session_config = {\n",
    "        \"threads\": 2,  # Add your code here\n",
    "        \"providers\": ['CPUExecutionProvider'],  # Add your code here\n",
    "        # ORT_ENABLE_ALL is also a typicalle safe good option for more aggressive inference; let's keep ORT_ENABLE_EXTENDED for easier debugging as a first run\n",
    "        \"optimization_level\": ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED,  # Add your code here\n",
    "        \"execution_mode\": ort.ExecutionMode.ORT_SEQUENTIAL  # Add your code here\n",
    "    }\n",
    "\n",
    "    # Create ONNX Runtime session\n",
    "    sess_options = ort.SessionOptions()\n",
    "    sess_options.inter_op_num_threads = session_config[\"threads\"]\n",
    "    sess_options.execution_mode = session_config[\"execution_mode\"]\n",
    "    sess_options.graph_optimization_level = session_config['optimization_level']\n",
    "    session = ort.InferenceSession(model_path, sess_options, providers=session_config['providers'])\n",
    "    \n",
    "    # Prepare input on CPU (simulating mobile environment)\n",
    "    sample_input_np = sample_input.numpy()\n",
    "    \n",
    "    # Get input name\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        session.run(None, {input_name: sample_input_np})\n",
    "    \n",
    "    # Benchmark\n",
    "    latencies = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        onnx_output = session.run(None, {input_name: sample_input_np})\n",
    "        latencies.append((time.time() - start) * 1000)  # ms\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # Compare outputs\n",
    "    onnx_output_tensor = torch.from_numpy(onnx_output[0])\n",
    "    is_output_close = torch.allclose(original_output, onnx_output_tensor, rtol=1e-3, atol=1e-5)\n",
    "    max_accuracy_diff = (original_output - onnx_output_tensor).abs().max().item()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"framework\": \"ONNX Runtime\",\n",
    "        \"accuracy_check\": is_output_close,\n",
    "        \"accuracy_diff\": max_accuracy_diff,\n",
    "        \"avg_latency_ms\": np.mean(latencies),\n",
    "        \"p90_latency_ms\": np.percentile(latencies, 90),\n",
    "        \"max_latency_ms\": np.max(latencies),\n",
    "        \"min_latency_ms\": np.min(latencies),\n",
    "        \"throughput\": n_runs / total_time,\n",
    "        \"model_size_mb\": os.path.getsize(model_path) / (1024 * 1024)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Benchmark both frameworks\n",
    "print(\"Benchmarking ExecuTorch model...\")\n",
    "executorch_metrics = benchmark_executorch(executorch_path, sample_input)\n",
    "\n",
    "print(\"Benchmarking ONNX Runtime model...\")\n",
    "onnx_metrics = benchmark_onnx(optimized_onnx_path, sample_input)\n",
    "\n",
    "# Compare results\n",
    "results_df = pd.DataFrame([executorch_metrics, onnx_metrics])\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(results_df.set_index('framework'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why is ExecuTorch performing so poorly?** ExecuTorch is designed for mobile and edge devices, not desktop CPUs. Running it on a development machine—especially without hardware acceleration—can result in much slower performance compared to ONNX Runtime, which is optimized for desktop/server inference. Always test on representative target devices rather than relying solely on development machines. \n",
    "> <br> And, when it comes to mobile, test on a variety of devices! This is because performance varies significantly across device models: high-end phones might show minimal differences between frameworks, while budget devices often reveal larger gaps. \n",
    "> \n",
    "> Running the inference benchmark here is just to demo the functionality. You could consider using [ExecuTorch with CMake for more advanced profiling](https://docs.pytorch.org/executorch/stable/tutorial-xnnpack-delegate-lowering.html#running-the-xnnpack-model-with-cmake).\n",
    ">\n",
    "> **Brainstorming question**: Different environments have different capabilities. How would you set the session configuration `session_config` for cloud and edge?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze mobile-specific considerations\n",
    "\n",
    "Let's analyze some mobile-specific considerations for each framework that impact deployment decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mobile-Specific Considerations:\n",
      "                 Category  \\\n",
      "0         App Size Impact   \n",
      "1           Battery Usage   \n",
      "2  Integration Complexity   \n",
      "3   Runtime Compatibility   \n",
      "4      Update Flexibility   \n",
      "\n",
      "                                                    ExecuTorch  \\\n",
      "0  May increase app size due to bundled PyTorch Mobile runtime   \n",
      "1             Depends on backend optimizations (e.g., XNNPACK)   \n",
      "2           Easier if already using PyTorch; tight integration   \n",
      "3                  Requires PyTorch Mobile runtime integration   \n",
      "4             Supports updating .pte files without app rebuild   \n",
      "\n",
      "                                              ONNX Runtime  \n",
      "0      Smaller runtime possible; better for minimal builds  \n",
      "1     Also backend-dependent; supports efficient threading  \n",
      "2    More flexible; supports multiple languages/frameworks  \n",
      "3                 Requires packaging ONNX Runtime with app  \n",
      "4  Model updates require ONNX export + possible conversion  \n"
     ]
    }
   ],
   "source": [
    "# Ensure the output is fully printed \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def analyze_mobile_considerations(executorch_metrics, onnx_metrics):\n",
    "    \"\"\"Analyze mobile-specific considerations for both frameworks.\"\"\"\n",
    "    # Calculate differences\n",
    "    latency_diff_pct = ((executorch_metrics[\"avg_latency_ms\"] - onnx_metrics[\"avg_latency_ms\"]) / \n",
    "                      onnx_metrics[\"avg_latency_ms\"]) * 100\n",
    "    size_diff_pct = ((executorch_metrics[\"model_size_mb\"] - onnx_metrics[\"model_size_mb\"]) / \n",
    "                   onnx_metrics[\"model_size_mb\"]) * 100\n",
    "    \n",
    "    # Create comparison table\n",
    "    considerations = {\n",
    "        \"Category\": [\n",
    "            \"App Size Impact\", \n",
    "            \"Battery Usage\", \n",
    "            \"Integration Complexity\",\n",
    "            \"Runtime Compatibility\",\n",
    "            \"Update Flexibility\"\n",
    "        ],\n",
    "        \"ExecuTorch\": [\n",
    "            \"May increase app size due to bundled PyTorch Mobile runtime\",\n",
    "            \"Depends on backend optimizations (e.g., XNNPACK)\",\n",
    "            \"Easier if already using PyTorch; tight integration\",\n",
    "            \"Requires PyTorch Mobile runtime integration\",\n",
    "            \"Supports updating .pte files without app rebuild\"\n",
    "        ],\n",
    "        \"ONNX Runtime\": [\n",
    "            \"Smaller runtime possible; better for minimal builds\",\n",
    "            \"Also backend-dependent; supports efficient threading\",\n",
    "            \"More flexible; supports multiple languages/frameworks\",\n",
    "            \"Requires packaging ONNX Runtime with app\",\n",
    "            \"Model updates require ONNX export + possible conversion\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return  pd.DataFrame(considerations)\n",
    "\n",
    "# Analyze mobile considerations\n",
    "considerations_df = analyze_mobile_considerations(executorch_metrics, onnx_metrics)\n",
    "print(\"\\nMobile-Specific Considerations:\")\n",
    "print(considerations_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Beyond performance: Mobile ML success factors**: While benchmarks provide valuable data, successful mobile ML deployment requires considering many other factors:\n",
    "> \n",
    "> - *First-run latency*: Initial startup time can be significantly longer than steady-state\n",
    "> - *Memory footprint*: Runtime memory usage (not just model size) affects app stability\n",
    "> - *Battery consumption*: Especially important for background processing\n",
    "> - *App size increase*: Each MB added to your app reduces installation rates (estimated at ~0.5% loss per MB)\n",
    "> - *OS compatibility*: Newer ML features may require recent OS versions\n",
    "> - *Versioning strategy*: How you'll update models without requiring app updates\n",
    "> \n",
    "> These considerations often outweigh small performance differences between frameworks. A slightly slower model that uses less memory or enables easier updates may be preferable in real-world scenarios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you've learned how to:\n",
    "\n",
    "- Export a pre-trained MobileNetV3 model to both ExecuTorch and ONNX formats\n",
    "- Optimize these models specifically for mobile deployment\n",
    "- Benchmark and compare performance across different deployment frameworks\n",
    "- Analyze mobile-specific considerations like battery usage and app size\n",
    "- Make data-driven decisions about deployment frameworks and optimization techniques\n",
    "\n",
    "These skills are essential for mobile ML engineers working on vision applications where efficient deployment is critical for user experience. By understanding the trade-offs between different frameworks and optimization techniques, you can make informed decisions for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
