{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdaciSense: Optimized Object Recognition\n",
    "\n",
    "## Notebook 4: Mobile Deployment\n",
    "\n",
    "In this notebook, you'll prepare your optimized model for mobile deployment.\n",
    "You'll explore how to convert your best optimized model to a cross-platform mobile-friendly format,\n",
    "and evaluate the performance that UdaciSense mobile users can expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that libraries are dynamically re-loaded if changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import copy\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.quantization\n",
    "from typing import Dict, Any, List, Tuple, Optional, Union, Callable\n",
    "import warnings\n",
    "\n",
    "from utils.data_loader import get_household_loaders, get_input_size, print_dataloader_stats, visualize_batch\n",
    "from utils.model import MobileNetV3_Household, load_model, save_model, print_model_summary\n",
    "from utils.visualization import (\n",
    "    plot_model_comparison, plot_multiple_models_comparison, \n",
    "    create_model_summary_dashboard\n",
    ")\n",
    "from utils.compression import is_quantized\n",
    "from utils.evaluation import evaluate_model_metrics, compare_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore PyTorch deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Optional: Ignore all user warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "devices = [\"cpu\"]\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    devices.extend([f\"cuda:{i} ({torch.cuda.get_device_name(i)})\" for i in range(num_devices)])\n",
    "print(f\"Devices available: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "os.makedirs(\"../models/mobile\", exist_ok=True)\n",
    "os.makedirs(\"../results/mobile\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_deterministic_mode(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = seed + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "    \n",
    "    return seed_worker\n",
    "\n",
    "set_deterministic_mode(42)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load household objects dataset\n",
    "train_loader, test_loader = get_household_loaders(\n",
    "    image_size=\"CIFAR\", batch_size=128, num_workers=2,\n",
    ")\n",
    "\n",
    "# Get input_size\n",
    "input_size = get_input_size(\"CIFAR\")\n",
    "print(f\"Input has size: {input_size}\")\n",
    "\n",
    "# Get class names\n",
    "class_names = train_loader.dataset.classes\n",
    "print(f\"Datasets have these classes: \")\n",
    "for i in range(len(class_names)):\n",
    "    print(f\"  {i}: {class_names[i]}\")\n",
    "\n",
    "# Visualize some examples\n",
    "for dataset_type, data_loader in [('train', train_loader), ('test', test_loader)]:\n",
    "    print(f\"\\nInformation on {dataset_type} set\")\n",
    "    print_dataloader_stats(data_loader, dataset_type)\n",
    "    print(f\"Examples of images from the {dataset_type} set\")\n",
    "    visualize_batch(data_loader, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Load the optimized model and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose the experiment to load for\n",
    "experiment_name = \"1_post-global-pruning_dynamic-quant_torchscript\"\n",
    "optimized_model_path = f\"../models/pipeline/{experiment_name}/model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimized model\n",
    "optimized_model = load_model(optimized_model_path)\n",
    "print_model_summary(optimized_model)\n",
    "\n",
    "# Load optimized model metrics\n",
    "with open(f\"../results/pipeline/{experiment_name}/pipeline_metrics.json\", \"r\") as f:\n",
    "    optimized_metrics = json.load(f)\n",
    "\n",
    "print(\"\\nOptimized Model Metrics:\")\n",
    "pprint.pp(optimized_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Convert optimized model for mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement any required torch optimizations for deployment to mobile\n",
    "# Review built-in functionalities for mobile (e.g., mobile_optimizer)\n",
    "def convert_model_for_mobile(\n",
    "    model: nn.Module,\n",
    "    input_size: Tuple[int, ...] = (1, 3, 32, 32)\n",
    ") -> Union[torch.jit.ScriptModule, str]:\n",
    "    \"\"\"\n",
    "    Convert a PyTorch model to a mobile-friendly format.\n",
    "    Currently focused on TorchScript but designed to be extensible.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to convert\n",
    "        input_size: Shape of input tensor (possibly useful to create a dummy input)\n",
    "    Returns:\n",
    "        Converted model object\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model for mobile deployment\n",
    "print(\"\\nConverting model for mobile deployment...\")\n",
    "\n",
    "# Convert the model\n",
    "mobile_model = convert_model_for_mobile(\n",
    "    optimized_model, \n",
    "    input_size=input_size,\n",
    "    mobile_optimize=True,\n",
    ")\n",
    "\n",
    "# Save the mobile model\n",
    "# EXTRA: Test saving with lite interpreter instead - do you notice differences in performance?\n",
    "mobile_model_path = f\"../models/mobile/optimized_model_mobile.pt\"\n",
    "torch.jit.save(mobile_model, mobile_model_path)\n",
    "print(f\"Saved mobile-compatible model to: {mobile_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Verify Mobile Model Performance\n",
    "\n",
    "Before packaging for deployment, let's verify that your optimized model meets the requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model output consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement checks to guarantee the same model outputs\n",
    "# Once again, you can look at built-in PyTorch functionalities for inspiration\n",
    "# Also consider which device to perform the operations on\n",
    "def compare_model_outputs(\n",
    "    model1: nn.Module,\n",
    "    model2: nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Compare outputs of two models to verify consistency after conversion.\n",
    "    Works with both regular PyTorch models and converted mobile models.\n",
    "    \n",
    "    Args:\n",
    "        model1: First model\n",
    "        model2: Second model\n",
    "        input_tensor: Input tensor to test with\n",
    "        \n",
    "    Returns:\n",
    "        True if outputs are consistent, False otherwise\n",
    "    \"\"\"\n",
    "    pass\n",
    "        \n",
    "# Verify model output consistency\n",
    "dummy_input = torch.randn(input_size)\n",
    "output_consistency = compare_model_outputs(optimized_model, mobile_model, dummy_input)\n",
    "print(f\"Output consistency check: {'PASSED' if output_consistency else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model size\n",
    "# Consider if you want to return both or either model size in MB vs parameter count\n",
    "def get_model_size(model_path: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the size of a model, whether it is optimized or not for mobile.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Filepath to the saved model\n",
    "        \n",
    "    Returns:\n",
    "        Float number representing model size\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "optimized_size = get_model_size(optimized_model_path)\n",
    "mobile_size = get_model_size(mobile_model_path)\n",
    "\n",
    "print(f\"Original model size: {optimized_size:.2f} MB\")\n",
    "print(f\"Mobile model size: {mobile_size:.2f} MB\")\n",
    "print(f\"\\nSize change from optimized to mobile: {(mobile_size - optimized_size) / optimized_size * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate models on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluate original optimized vs mobile model when it comes to accuracy and other performance metrics\n",
    "# Feel free to choose one or more of the built-in evaluation and visualization methods, or create new ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark Mobile Performance\n",
    "\n",
    "In this section, you should develop a strategy for benchmarking the model on actual mobile \n",
    "hardware. Since we can't easily test on ARM mobile devices in this environment, describe:\n",
    "\n",
    "1. What tools and frameworks you would use for mobile benchmarking?\n",
    "2. What specific metrics you would collect?\n",
    "3. How you would set up a fair comparison between models?\n",
    "4. What mobile-specific factors you would control for in your tests?\n",
    "\n",
    "Write your benchmarking approach in the final report directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "**TODO: Collect results on mobile conversion and considerations for model deployment**\n",
    "\n",
    "After converting your optimized model for mobile deployment, analyze how the model will perform in real-world mobile environments.\n",
    "\n",
    "Consider these guiding questions:\n",
    "- How did the mobile conversion affect the model's performance characteristics?\n",
    "- What mobile-specific considerations impact this model's deployment?\n",
    "- How would you rigorously benchmark performance across different devices?\n",
    "- What challenges might arise in production deployment scenarios?\n",
    "- What future improvements would you prioritize for model deployment?\n",
    "\n",
    "Provide an analysis that demonstrates your understanding of mobile deployment considerations for the UdaciSense computer vision model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Model Mobile Deployment Analysis for UdaciSense Computer Vision Model\n",
    "\n",
    "*Replace this with your analysis*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸš€ **Next Step:** \n",
    "> Collect all your results and insights in `report.md`! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
