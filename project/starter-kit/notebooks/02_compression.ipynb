{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdaciSense: Optimized Object Recognition\n",
    "## Notebook 2: Compression Techniques\n",
    "\n",
    " \n",
    "In this notebook, you'll explore different model compression techniques to meet the requirements:\n",
    "- The optimized model should be **30% smaller** than the baseline\n",
    "- The optimized model should **reduce inference time by 40%**\n",
    "- The optimized model should **maintain accuracy within 5%** of the baseline\n",
    "\n",
    "You can experiment with different methods:\n",
    "1. **Post-training**: Quantization, pruning, graph optimizations.\n",
    "2. **In-training**: Quantization, pruning, distillation.\n",
    "Optionally, you can choose to implement other techniques too.\n",
    "\n",
    "Make sure to experiment with at least two different techniques. \n",
    "You will need to combine the selected techniques into a multi-step compression pipeline next, so make sure to select techniques that seem  promising individually but also combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that libraries are dynamically re-loaded if changed\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from compression.post_training.pruning import prune_model\n",
    "from compression.post_training.quantization import quantize_model\n",
    "from compression.post_training.graph_optimization import optimize_model, verify_model_equivalence\n",
    "from compression.in_training.distillation import train_with_distillation, MobileNetV3_Household_Small\n",
    "from compression.in_training.pruning import train_with_pruning\n",
    "from compression.in_training.quantization import train_model_qat, QuantizableMobileNetV3_Household\n",
    "\n",
    "from utils import MAX_ALLOWED_ACCURACY_DROP, TARGET_INFERENCE_SPEEDUP,TARGET_MODEL_COMPRESSION \n",
    "from utils.data_loader import get_household_loaders, get_input_size, print_dataloader_stats, visualize_batch\n",
    "from utils.model import MobileNetV3_Household, load_model, save_model, print_model_summary\n",
    "from utils.visualization import plot_multiple_models_comparison\n",
    "from utils.compression import (\n",
    "    compare_experiments, compare_optimized_model_to_baseline, evaluate_optimized_model, list_experiments,  # For experimentation\n",
    "    is_quantized  # For quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore PyTorch deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Optional: Ignore all user warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "devices = [\"cpu\"]\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    devices.extend([f\"cuda:{i} ({torch.cuda.get_device_name(i)})\" for i in range(num_devices)])\n",
    "print(f\"Devices available: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for each technique\n",
    "compression_types = [\n",
    "    \"post_training/pruning\",\n",
    "    \"post_training/quantization\",\n",
    "    \"post_training/graph_optimization\",\n",
    "    \"in_training/distillation\", \n",
    "    \"in_training/quantization\",\n",
    "    \"in_training/pruning\",\n",
    "]\n",
    "for comp_type in compression_types:\n",
    "    models_dir = f\"../models/{comp_type}\"\n",
    "    models_ckp_dir = f\"{models_dir}/checkpoints\"\n",
    "    results_dir = f\"../results/{comp_type}\"\n",
    "    \n",
    "    os.makedirs(models_ckp_dir, exist_ok=True)\n",
    "    os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_deterministic_mode(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = seed + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "    \n",
    "    return seed_worker\n",
    "\n",
    "set_deterministic_mode(42)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load household objects dataset\n",
    "train_loader, test_loader = get_household_loaders(\n",
    "    image_size=\"CIFAR\", batch_size=128, num_workers=2,\n",
    ")\n",
    "\n",
    "# Get input_size\n",
    "input_size = get_input_size(\"CIFAR\")\n",
    "print(f\"Input has size: {input_size}\")\n",
    "\n",
    "# Get class names\n",
    "class_names = train_loader.dataset.classes\n",
    "print(f\"Datasets have these classes: \")\n",
    "for i in range(len(class_names)):\n",
    "    print(f\"  {i}: {class_names[i]}\")\n",
    "\n",
    "# Visualize some examples\n",
    "for dataset_type, data_loader in [('train', train_loader), ('test', test_loader)]:\n",
    "    print(f\"\\nInformation on {dataset_type} set\")\n",
    "    print_dataloader_stats(data_loader, dataset_type)\n",
    "    print(f\"Examples of images from the {dataset_type} set\")\n",
    "    visualize_batch(data_loader, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the baseline model and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the baseline model\n",
    "baseline_model = MobileNetV3_Household()\n",
    "baseline_model_name = \"baseline_mobilenet\"\n",
    "baseline_model.load_state_dict(torch.load(f\"../models/{baseline_model_name}/checkpoints/model.pth\"))\n",
    "print_model_summary(baseline_model)\n",
    "\n",
    "# Load baseline metrics\n",
    "with open(f\"../results/{baseline_model_name}/metrics.json\", \"r\") as f:\n",
    "    baseline_metrics = json.load(f)\n",
    "\n",
    "print(\"\\nBaseline Model Metrics:\")\n",
    "pprint.pp(baseline_metrics)\n",
    "\n",
    "# Calculate target metrics based on CTO requirements\n",
    "target_model_size = baseline_metrics['size']['model_size_mb'] * (1 - TARGET_MODEL_COMPRESSION)\n",
    "target_inference_time_cpu = baseline_metrics['timing']['cpu']['avg_time_ms'] * (1 - TARGET_INFERENCE_SPEEDUP)\n",
    "if torch.cuda.is_available():\n",
    "    target_inference_time_gpu = baseline_metrics['timing']['cuda']['avg_time_ms'] * (1 - TARGET_INFERENCE_SPEEDUP)\n",
    "min_acceptable_accuracy = baseline_metrics['accuracy']['top1_acc'] * (1 - MAX_ALLOWED_ACCURACY_DROP) \n",
    "\n",
    "print(\"Optimization Targets:\")\n",
    "print(f\"Target Model Size: {baseline_metrics['size']['model_size_mb']:.2f} --> {target_model_size:.2f} MB ({TARGET_MODEL_COMPRESSION*100}% reduction)\")\n",
    "print(f\"Target Inference Time (CPU): {baseline_metrics['timing']['cpu']['avg_time_ms']:.2f} --> {target_inference_time_cpu:.2f} ms ({TARGET_INFERENCE_SPEEDUP*100}% reduction)\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Target Inference Time (GPU): {baseline_metrics['timing']['cuda']['avg_time_ms']:.2f} --> {target_inference_time_gpu:.2f} ms ({TARGET_INFERENCE_SPEEDUP*100}% reduction)\")\n",
    "print(f\"Minimum Acceptable Accuracy: {baseline_metrics['accuracy']['top1_acc']:.2f} --> {min_acceptable_accuracy:.2f} (within {MAX_ALLOWED_ACCURACY_DROP*100}% of baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement and evaluate compression techniques\n",
    "\n",
    "Now you'll implement and evaluate different compression techniques. For each technique that you choose, you'll:\n",
    "1. Implement the technique\n",
    "2. Evaluate its impact on model size, inference time, and accuracy\n",
    "3. Analyze the trade-offs\n",
    "\n",
    "To choose a technique, simply uncomment the apply_{TECHNIQUE_NAME}_technique() function call in the corresponding technique cell block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Post-Training - Quantization\n",
    "\n",
    "Quantization reduces the precision of weights and activations, converting floating-point values to integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a function to apply quantization and evaluate results\n",
    "def apply_post_training_quantization(quantization_type, backend, device):\n",
    "    \"\"\"\n",
    "    Apply quantization to a model with given method and backend.\n",
    "    \n",
    "    Args:\n",
    "        quantization_type: Quantization method (\"static\" or \"dynamic\")\n",
    "        backend: Backend for quantization (\"fbgemm\" for x86 or \"qnnpack\" for ARM)\n",
    "        device: Which device to use for model loading, training, and evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (optimized_model, comparison_results, experiment_name)\n",
    "    \"\"\"\n",
    "    # Define unique experiment name given main parameters\n",
    "    experiment_name = f\"post_training/quantization/{quantization_type}\"\n",
    "    \n",
    "    # Create experiment subdirectories\n",
    "    os.makedirs(f\"../models/{experiment_name}\", exist_ok=True)\n",
    "    os.makedirs(f\"../results/{experiment_name}\", exist_ok=True)\n",
    "    \n",
    "    print(f\"Applying {quantization_type} quantization with {backend} backend\")\n",
    "    \n",
    "    # Make a copy of the baseline model and move to specified device\n",
    "    orig_model = load_model(f\"../models/{baseline_model_name}/checkpoints/model.pth\").to(device)\n",
    "    orig_model.eval()\n",
    "    \n",
    "    # Apply post-training quantization\n",
    "    # TODO: IMPLEMENT THIS FUNCTION IN THE compression/ FOLDER    \n",
    "    quantized_model = quantize_model(\n",
    "        orig_model,\n",
    "        quantization_type=quantization_type,\n",
    "        calibration_data_loader=train_loader if quantization_type == \"static\" else None,\n",
    "        calibration_num_batches=1 if quantization_type == \"static\" else None,  # Set this to the desired value\n",
    "        backend=backend,\n",
    "    )\n",
    "    \n",
    "    # Save the quantized model\n",
    "    save_model(quantized_model, f\"../models/{experiment_name}/model.pth\")\n",
    "    \n",
    "    # Check that model is indeed quantized\n",
    "    is_quantized(quantized_model)\n",
    "    \n",
    "    # Evaluate quantized model\n",
    "    evaluate_optimized_model(\n",
    "        quantized_model, test_loader, experiment_name, class_names, input_size, device=torch.device('cpu')\n",
    "    )\n",
    "    \n",
    "    # Compare with baseline model for performance differences\n",
    "    comparison_results = compare_optimized_model_to_baseline(\n",
    "        baseline_model,\n",
    "        quantized_model,\n",
    "        experiment_name,\n",
    "        test_loader,\n",
    "        class_names,\n",
    "        device=torch.device('cpu'),\n",
    "    )\n",
    "    \n",
    "    return quantized_model, comparison_results, experiment_name\n",
    "\n",
    "#### Apply post-training quantization\n",
    "## Find info at https://pytorch.org/docs/stable/quantization.html\n",
    "\n",
    "## TODO: Experiment with different configurations\n",
    "## Feel free to add more configuration parameters (and update the script in `compression/` folder accordingly)\n",
    "quantization_type = None # One of \"dynamic\" or \"static\"\n",
    "backend = None  # One of \"fbgemm\" or \"qnnpack\"\n",
    "device = None  # Define using torch.device()\n",
    "\n",
    "# Optimize and evaluate model\n",
    "quantized_model_static, quantized_comparison_results, experiment_name = apply_post_training_quantization(quantization_type, backend=backend, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 In-training - Quantization\n",
    "\n",
    "Quantization-aware training simulates quantization during training, allowing the model to adapt to the reduced precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply quantization-aware training and evaluate results\n",
    "def apply_quantization_aware_training(model, config, backend):\n",
    "    \"\"\"\n",
    "    Apply quantization-aware training to a model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model architecture to quantize\n",
    "        config: Dictionary containing the training configuration for the experiment\n",
    "        backend: Backend for quantization (\"fbgemm\" for x86 or \"qnnpack\" for ARM)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (optimized_model, comparison_results, experiment_name)\n",
    "    \"\"\"\n",
    "    #Â Extract relevant training parameters for logging\n",
    "    qat_start_epoch, num_epochs = config['qat_start_epoch'], config['num_epochs']\n",
    "    \n",
    "    # Define unique experiment name given main parameters\n",
    "    experiment_name = f\"in_training/quantization/epochs{num_epochs}_start{qat_start_epoch}\"\n",
    "    experiment_name = experiment_name.replace('.', '-')\n",
    "    \n",
    "    # Create experiment subdirectories\n",
    "    os.makedirs(f\"../models/{experiment_name}\", exist_ok=True)\n",
    "    os.makedirs(f\"../results/{experiment_name}\", exist_ok=True)\n",
    "    \n",
    "    print(f\"Applying quantization-aware training with QAT starting at epoch {qat_start_epoch} / ending at {num_epochs}\")\n",
    "        \n",
    "    # Move model to specified device\n",
    "    model = model.to(config['device'])\n",
    "    \n",
    "    # Train with QAT\n",
    "    # TODO: IMPLEMENT THIS FUNCTION IN THE compression/ FOLDER   \n",
    "    quantized_model, qat_stats, qat_best_accuracy, qat_best_epoch = train_model_qat(\n",
    "        model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        config,\n",
    "        checkpoint_path=f\"{os.getcwd()}/../models/{experiment_name}/checkpoints\",  # Providing full path to manage different sub-directory depths in utility scripts\n",
    "        backend=backend,\n",
    "    )\n",
    "    \n",
    "    # Save training statistics\n",
    "    with open(f\"../results/{experiment_name}/training_stats.json\", 'w') as f:\n",
    "        json.dump(qat_stats, f, indent=4)\n",
    "    \n",
    "    # Save the quantized model\n",
    "    save_model(quantized_model, f\"../models/{experiment_name}/model.pth\")\n",
    "    \n",
    "    # Check that model is indeed quantized\n",
    "    is_quantized(quantized_model)\n",
    "    \n",
    "    # Evaluate quantized model\n",
    "    metrics, confusion_matrix = evaluate_optimized_model(\n",
    "        quantized_model, \n",
    "        test_loader, \n",
    "        experiment_name,\n",
    "        class_names,\n",
    "        input_size,\n",
    "        is_in_training_technique=True,\n",
    "        training_stats=qat_stats,\n",
    "        device=config[\"device_for_inference\"],\n",
    "    )\n",
    "    \n",
    "    # Compare with baseline model for performance differences\n",
    "    comparison_results = compare_optimized_model_to_baseline(\n",
    "        baseline_model,\n",
    "        quantized_model,\n",
    "        experiment_name,\n",
    "        test_loader,\n",
    "        class_names,\n",
    "        device=config[\"device_for_inference\"],\n",
    "    )\n",
    "    \n",
    "    return quantized_model, comparison_results, experiment_name\n",
    "\n",
    "#### Apply quantization-aware training\n",
    "## Find info at https://pytorch.org/docs/stable/quantization.html\n",
    "\n",
    "##Â Create quantizable model version\n",
    "## TODO: Check the model implementation in the `compression/` folder\n",
    "model = QuantizableMobileNetV3_Household(quantize=False)\n",
    "    \n",
    "## TODO: Experiment with different configurations\n",
    "##Â We recommend testing testing with various start and freeze epochs\n",
    "## Feel free to add more configuration parameters (and update the script in `compression/` folder accordingly)\n",
    "config = {\n",
    "    'qat_start_epoch': None,  # Integer\n",
    "    'freeze_bn_epochs': None,  # Integer\n",
    "    'num_epochs': None,  # Integer\n",
    "    'criterion': None,  # A PyTorch loss\n",
    "    'optimizer': None,  #Â A PyTorch optimizer\n",
    "    'scheduler': None,  #Â A PyTorch scheduler\n",
    "    'patience': None,  # Integer\n",
    "    'device': None,  # Define with torch.device()\n",
    "    'device_for_inference': None,  # Define with torch.device()\n",
    "    'grad_clip_norm': None,  # Float\n",
    "}\n",
    "backend = None  # One of \"fbgemm\" or \"qnnpack\"\n",
    "\n",
    "# Optimize and evaluate model\n",
    "qat_model, qat_comparison_results, experiment_name = apply_quantization_aware_training(model, config, backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Post-training - Pruning\n",
    "\n",
    "Pruning reduces model size by removing weights with small magnitudes that contribute less to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply pruning and evaluate results\n",
    "def apply_post_training_pruning(config):\n",
    "    \"\"\"\n",
    "    Apply post-training pruning to a model with given pruning method and amount\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary containing the configuration for the experiment\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (optimized_model, comparison_results, experiment_name)\n",
    "    \"\"\"\n",
    "    #Â Extract relevant training parameters for logging\n",
    "    amount, pruning_method, device = config['amount'], config['pruning_method'], config['device']\n",
    "    \n",
    "    # Define unique experiment name given main parameters\n",
    "    experiment_name = f\"post_training/pruning/{pruning_method}_{amount}_{device}\"\n",
    "    experiment_name = experiment_name.replace('.', '-')\n",
    "    \n",
    "    # Create experiment subdirectories\n",
    "    os.makedirs(f\"../models/{experiment_name}\", exist_ok=True)\n",
    "    os.makedirs(f\"../results/{experiment_name}\", exist_ok=True)\n",
    "    \n",
    "    print(f\"Applying post-training pruning with method {pruning_method} and amount {amount:.2f}\")\n",
    "    \n",
    "    # Make a copy of the baseline model and move to specified device\n",
    "    orig_model = load_model(f\"../models/{baseline_model_name}/checkpoints/model.pth\").to(device)\n",
    "    orig_model.eval()\n",
    "    \n",
    "    # Apply post-training pruning\n",
    "    # TODO: IMPLEMENT THIS FUNCTION IN THE compression/ FOLDER \n",
    "    pruned_model = prune_model(orig_model, pruning_method, amount, config[\"modules_to_prune\"], config[\"custom_pruning_fn\"])\n",
    "    \n",
    "    # Save the pruned model\n",
    "    save_model(pruned_model, f\"../models/{experiment_name}/model.pth\")\n",
    "    \n",
    "    # Evaluate pruned model\n",
    "    metrics, confusion_matrix = evaluate_optimized_model(\n",
    "        pruned_model, \n",
    "        test_loader, \n",
    "        experiment_name,\n",
    "        class_names,\n",
    "        input_size,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    # Compare with baseline model for performance differences\n",
    "    comparison_results = compare_optimized_model_to_baseline(\n",
    "        baseline_model,\n",
    "        pruned_model,\n",
    "        experiment_name,\n",
    "        test_loader,\n",
    "        class_names,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    return pruned_model, comparison_results, experiment_name\n",
    "\n",
    "\n",
    "# Apply post-training pruning    \n",
    "## Find info at https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\n",
    "\n",
    "## TODO: Experiment with different configurations\n",
    "## We recommend testing pruning with various ratios\n",
    "## Feel free to add more configuration parameters (and update the script in `compression/` folder accordingly)\n",
    "config = {\n",
    "    'pruning_method': None,  # String\n",
    "    'amount': None,  # Float\n",
    "    'modules_to_prune': None,  # (Optional) List\n",
    "    'n': None,  # (Optional: Used for ln_structured pruning) Int\n",
    "    'dim': None,  # Optional: Used for ln_structured pruning) Int\n",
    "    'custom_pruning_fn': None,  # (Optional) Fn\n",
    "    'device': None,  # Define with torch.device()\n",
    "}\n",
    "\n",
    "# Optimize and evaluate model\n",
    "pruned_model, pruned_results, experiment_name = apply_post_training_pruning(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 In-training - Pruning\n",
    "\n",
    "Gradual pruning progressively prunes weights during training, allowing the model to adapt to increasing sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a function to apply pruning during training and evaluate results\n",
    "def apply_in_training_pruning(model, config):\n",
    "    \"\"\"\n",
    "    Apply gradual pruning during training.\n",
    "    \n",
    "    Args:\n",
    "        model: The model architecture to quantize\n",
    "        config: Dictionary containing the training configuration for the experiment\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (optimized_model, comparison_results, experiment_name)\n",
    "    \"\"\"\n",
    "    #Â Extract relevant training parameters for logging\n",
    "    pruning_method, initial_sparsity, final_sparsity = config['pruning_method'], config['initial_sparsity'], config['final_sparsity'] \n",
    "    start_epoch, end_epoch = config['start_epoch'], config['end_epoch'] \n",
    "    device = config['device']\n",
    "    \n",
    "    # Define unique experiment name given main parameters\n",
    "    experiment_name = f\"in_training/pruning/{pruning_method}_sparsity{initial_sparsity}-{final_sparsity}_epochs{start_epoch}-{end_epoch}\"\n",
    "    experiment_name = experiment_name.replace('.', '-')\n",
    "    \n",
    "    # Create experiment subdirectories\n",
    "    os.makedirs(f\"../models/{experiment_name}\", exist_ok=True)\n",
    "    os.makedirs(f\"../results/{experiment_name}\", exist_ok=True)\n",
    "    \n",
    "    print(f\"Applying gradual pruning from {initial_sparsity:.1%} to {final_sparsity:.1%} sparsity\")\n",
    "    \n",
    "    # Move model to specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Train with gradual pruning\n",
    "    # TODO: IMPLEMENT THIS FUNCTION IN THE compression/ FOLDER \n",
    "    pruned_model, pruning_stats, pruned_best_accuracy, pruned_best_epoch = train_with_pruning(\n",
    "        model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        config,\n",
    "        checkpoint_path=f\"../models/{experiment_name}/model.pth\"\n",
    "    )\n",
    "    \n",
    "    # Save training statistics\n",
    "    with open(f\"../results/{experiment_name}/training_stats.json\", 'w') as f:\n",
    "        json.dump(pruning_stats, f, indent=4)\n",
    "    \n",
    "    # Save the quantized model\n",
    "    save_model(pruned_model, f\"../models/{experiment_name}/model.pth\")\n",
    "    \n",
    "    # Evaluate quantized model\n",
    "    metrics, confusion_matrix = evaluate_optimized_model(\n",
    "        pruned_model, \n",
    "        test_loader, \n",
    "        experiment_name,\n",
    "        class_names,\n",
    "        input_size,\n",
    "        is_in_training_technique=True,\n",
    "        training_stats=pruning_stats,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    # Compare with baseline model for performance differences\n",
    "    comparison_results = compare_optimized_model_to_baseline(\n",
    "        baseline_model,\n",
    "        pruned_model,\n",
    "        experiment_name,\n",
    "        test_loader,\n",
    "        class_names,\n",
    "        device=device,\n",
    "    )\n",
    "    return pruned_model, comparison_results, experiment_name\n",
    "\n",
    "# Apply in-training pruning \n",
    "## Find info at https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\n",
    "\n",
    "##Â Create a new model instance\n",
    "model = MobileNetV3_Household()\n",
    "\n",
    "## TODO: Experiment with different configurations\n",
    "# We recommend testing pruning with various sparsity and epochs settings\n",
    "## Feel free to add more configuration parameters (and update the script in `compression/` folder accordingly)\n",
    "config = {\n",
    "    # General training config\n",
    "    'num_epochs': None,  # Integer\n",
    "    'criterion': None,  # A PyTorch loss  \n",
    "    'optimizer': None,  # A PyTorch optimizer   \n",
    "    'scheduler': None,  # A PyTorch scheduler\n",
    "    'patience': None,  # Integer\n",
    "    'device': None,  # Define with torch.device()\n",
    "    'grad_clip_norm': None,  # Float\n",
    "    # Pruning-specific config\n",
    "    'initial_sparsity': None,  # Float\n",
    "    'final_sparsity': None,    # Float\n",
    "    'start_epoch': None,  # Integer\n",
    "    'end_epoch': None,  # Integer  \n",
    "    'pruning_frequency': None,  # Integer\n",
    "    'pruning_method': None,  # String\n",
    "    'schedule_type': None,  # String\n",
    "    'only_prune_conv': None,  # Boolean\n",
    "}\n",
    "    \n",
    "pruned_model, pruned_comparison_results, experiment_name = apply_in_training_pruning(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 In-training - Knowledge Distillation\n",
    "\n",
    "Knowledge distillation trains a smaller student model to mimic a larger teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a function to apply knowledge distillation and evaluate results\n",
    "def apply_knowledge_distillation(teacher_model, student_model, config):\n",
    "    \"\"\"\n",
    "    Apply knowledge distillation from a teacher model to a student model.\n",
    "    \n",
    "    Args:\n",
    "        teacher_model: Pre-trained teacher model\n",
    "        student_model: Smaller student model to train\n",
    "        config: Dictionary containing the training configuration for the experiment\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (distilled_student_model, comparison_results, experiment_name)\n",
    "    \"\"\"\n",
    "    # Extract relevant training parameters for logging\n",
    "    temperature, alpha = config['temperature'], config['alpha']\n",
    "    num_epochs = config['num_epochs']\n",
    "    device = config['device']\n",
    "    \n",
    "    # Define unique experiment name given main parameters\n",
    "    experiment_name = f\"in_training/distillation/temp{temperature}_alpha{alpha}_epochs{num_epochs}\"\n",
    "    experiment_name = experiment_name.replace('.', '-')\n",
    "    \n",
    "    # Create experiment subdirectories\n",
    "    os.makedirs(f\"../models/{experiment_name}\", exist_ok=True)\n",
    "    os.makedirs(f\"../results/{experiment_name}\", exist_ok=True)\n",
    "    \n",
    "    print(f\"Applying knowledge distillation with temperature={temperature} and alpha={alpha}\")\n",
    "    \n",
    "    # Move models to specified device\n",
    "    teacher_model = teacher_model.to(device)\n",
    "    student_model = student_model.to(device)\n",
    "    \n",
    "    # Train student with knowledge distillation\n",
    "    # TODO: IMPLEMENT THIS FUNCTION IN THE compression/ FOLDER \n",
    "    distilled_model, distillation_stats, best_accuracy, best_epoch = train_with_distillation(\n",
    "        student_model,\n",
    "        teacher_model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        config,\n",
    "        checkpoint_path=f\"../models/{experiment_name}/model.pth\"\n",
    "    )\n",
    "    \n",
    "    # Save training statistics\n",
    "    with open(f\"../results/{experiment_name}/training_stats.json\", 'w') as f:\n",
    "        json.dump(distillation_stats, f, indent=4)\n",
    "    \n",
    "    # Save the distilled student model\n",
    "    save_model(distilled_model, f\"../models/{experiment_name}/model.pth\")\n",
    "    \n",
    "    # Evaluate distilled student model\n",
    "    metrics, confusion_matrix = evaluate_optimized_model(\n",
    "        distilled_model, \n",
    "        test_loader, \n",
    "        experiment_name,\n",
    "        class_names,\n",
    "        input_size,\n",
    "        is_in_training_technique=True,\n",
    "        training_stats=distillation_stats,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    # Compare with baseline model for performance differences\n",
    "    comparison_results = compare_optimized_model_to_baseline(\n",
    "        baseline_model,\n",
    "        distilled_model,\n",
    "        experiment_name,\n",
    "        test_loader,\n",
    "        class_names,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    return distilled_model, comparison_results, experiment_name\n",
    "\n",
    "# Apply knowledge distillation\n",
    "## Find more info at https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html\n",
    "\n",
    "## Load the pre-trained teacher model\n",
    "teacher_model = load_model(f\"../models/{baseline_model_name}/checkpoints/model.pth\")\n",
    "teacher_model.eval()  # Teacher should be in eval mode\n",
    "\n",
    "## Create student model\n",
    "## TODO: Check the model implementation in the `compression/` folder\n",
    "student_model = MobileNetV3_Household_Small(num_classes=len(class_names))\n",
    "## Uncomment print below to inspect the student model architecture\n",
    "# print_model_summary(student_model)\n",
    "\n",
    "# TODO: EXPERIMENT WITH DIFFERENT TRAINING PARAMETERS\n",
    "# We recommend testing distillation with different alpha and temperature\n",
    "## Feel free to add more configuration parameters (and update the script in `compression/` folder accordingly)\n",
    "optimizer = None  # A PyTorch optimizer\n",
    "config = {\n",
    "    'num_epochs': None,  # Integer\n",
    "    'criterion': None,  # A PyTorch loss\n",
    "    'optimizer': None,  # A PyTorch optimizer\n",
    "    'scheduler': None,  # A PyTorch scheduler\n",
    "    'alpha': None,  # Float\n",
    "    'temperature': None,  # Float\n",
    "    'patience': None,  # Integer\n",
    "    'device':  None,  # Define with torch.device()\n",
    "}\n",
    "distilled_model, distilled_comparison_metrics, experiment_name = apply_knowledge_distillation(teacher_model, student_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 In-training - Graph Optimizations\n",
    "\n",
    "Graph optimizations fuse operations and remove redundant nodes for better inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply graph optimization and evaluate results\n",
    "def apply_graph_optimization(optimization_method, input_shape=(1, 3, 32, 32), device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Apply graph optimization to a model.\n",
    "    \n",
    "    Args:\n",
    "        optimization_method: Optimization method to use in [\"torchscript\", \"torch_fx\"]\n",
    "        input_shape: Shape of input tensor\n",
    "        device: Which device to optimize the model on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (optimized_model, comparison_results, experiment_name)\n",
    "    \"\"\"\n",
    "    # Check optimization method is supported\n",
    "    if optimization_method not in [\"torchscript\", \"torch_fx\"]:\n",
    "        raise ValueError(f\"Unsupported optimization method: {optimization_method}\")\n",
    "    \n",
    "    # Define unique experiment name given main parameters\n",
    "    experiment_name = f\"post_training/graph_optimization/{optimization_method}_{device}\"\n",
    "\n",
    "    # Create experiment subdirectories\n",
    "    os.makedirs(f\"../models/{experiment_name}\", exist_ok=True)\n",
    "    os.makedirs(f\"../results/{experiment_name}\", exist_ok=True)\n",
    "    \n",
    "    print(f\"Applying optimization with {optimization_method} as method\")\n",
    "    \n",
    "    # Make a copy of the baseline model and move to specified device\n",
    "    orig_model = load_model(f\"../models/{baseline_model_name}/checkpoints/model.pth\").to(device)\n",
    "    \n",
    "    # Apply graph optimization\n",
    "    # TODO: IMPLEMENT THIS FUNCTION IN THE compression/ FOLDER \n",
    "    optimized_model = optimize_model(\n",
    "        orig_model,\n",
    "        optimization_method=optimization_method,\n",
    "        input_shape=input_shape,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    # Save the optimized model\n",
    "    file_extension = \".pth\" if optimization_method==\"torch_fx\" else \".pt\"\n",
    "    save_model(optimized_model, f\"../models/{experiment_name}/model{file_extension}\")\n",
    "    \n",
    "    # Verify model equivalence\n",
    "    is_equivalent = verify_model_equivalence(\n",
    "        orig_model, \n",
    "        optimized_model, \n",
    "        input_shape=input_shape, \n",
    "        device=device\n",
    "    )\n",
    " \n",
    "    # Evaluate quantized model\n",
    "    metrics, confusion_matrix = evaluate_optimized_model(\n",
    "        optimized_model, \n",
    "        test_loader, \n",
    "        experiment_name,\n",
    "        class_names,\n",
    "        input_size,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    # Compare with baseline model for performance differences\n",
    "    comparison_results = compare_optimized_model_to_baseline(\n",
    "        baseline_model,\n",
    "        optimized_model,\n",
    "        experiment_name,\n",
    "        test_loader,\n",
    "        class_names,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    return optimized_model, comparison_results, experiment_name\n",
    "\n",
    "# Apply graph optimization\n",
    "## Find info at https://pytorch.org/docs/stable/fx.html and https://pytorch.org/docs/stable/jit.html\n",
    "##Â NOTE: The model size estimation with torchscript is not accurate, you can expect a very similar model size to the original model    \n",
    "\n",
    "## TODO: EXPERIMENT WITH DIFFERENT  PARAMETERS\n",
    "##Â We recommend testing testing with both optimization methods and device types\n",
    "## Feel free to add more configuration parameters (and update the script in `compression/` folder accordingly)\n",
    "optimization_method = None  # One of \"torch_fx\" or \"torchscript\"\n",
    "device = None  # Define with torch.device()\n",
    "\n",
    "# Optimize and evaluate model\n",
    "graph_optimized_model, graph_comparison_results, experiment_name = apply_graph_optimization(optimization_method, input_shape=input_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare All Techniques\n",
    "\n",
    "Now, let's compare the techniques you've implemented to see which one(s) best meet the requirements.\n",
    "\n",
    "First, you can review all the experiments results stored locally and then you can define your preferred list of experiment names to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all experiments you've run to completion\n",
    "list_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of experiments to compare\n",
    "experiments_to_load_from_disk = list_experiments()\n",
    "experiments_to_load_from_memory = None\n",
    "\n",
    "experiments = (experiments_to_load_from_disk or []) + (experiments_to_load_from_memory or [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Or with a mix of pre-loaded and disk-based results\n",
    "_ = compare_experiments(\n",
    "    experiments=experiments,\n",
    "    baseline_metrics=baseline_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "**TODO: Analyze compression results and collect considerations on combining techniques for the multi-step pipeline**\n",
    "\n",
    "After implementing and testing various compression techniques, analyze your experimental results to identify the most effective approaches for the UdaciSense application.\n",
    "\n",
    "Consider these guiding questions:\n",
    "- How do different techniques affect the three key metrics (size, speed, accuracy)?\n",
    "- What technique-specific challenges or insights did you discover?\n",
    "- Which techniques show complementary strengths and weaknesses?\n",
    "- How could combining these techniques meet all CTO requirements?\n",
    "\n",
    "Provide a comparative analysis that leads to considerations for the multi-stage optimization pipeline you'll implement in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression Techniques Analysis for UdaciSense Object Recognition Model\n",
    "\n",
    "*(Replace this with your analysis)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸš€ **Next Step:** \n",
    "> Implement the multi-step optimization pipeline you've designed in notebook `03_pipeline.ipynb`  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
