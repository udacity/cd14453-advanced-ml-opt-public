{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdaciSense: Optimized Object Recognition\n",
    "\n",
    "## Notebook 3: Multi-step Optimization Pipeline\n",
    "\n",
    " \n",
    "In this notebook, you'll implement the multi-step optimization pipeline based on the findings from the previous experiments. The goal is to combine different optimization techniques to meet all requirements:\n",
    "\n",
    "- The optimized model should be **30% smaller** than the baseline\n",
    "- The optimized model should **reduce inference time by 40%**\n",
    "- The optimized model should **maintain accuracy within 5%** of the baseline\n",
    "\n",
    "You may need to experiment with different pipelines as you try to hit your targets. Make sure to start with those that are easier to implement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview: Implementation Plan\n",
    "\n",
    "**TODO**: *Add your implementation plan here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that libraries are dynamically re-loaded if changed\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from compression.post_training.pruning import prune_model\n",
    "from compression.post_training.quantization import quantize_model\n",
    "from compression.post_training.graph_optimization import optimize_model, verify_model_equivalence\n",
    "from compression.in_training.distillation import train_with_distillation, MobileNetV3_Household_Small\n",
    "from compression.in_training.pruning import train_with_pruning\n",
    "from compression.in_training.quantization import train_model_qat, QuantizableMobileNetV3_Household\n",
    "\n",
    "from utils import MAX_ALLOWED_ACCURACY_DROP, TARGET_INFERENCE_SPEEDUP, TARGET_MODEL_COMPRESSION\n",
    "from utils.data_loader import get_household_loaders, get_input_size, print_dataloader_stats, visualize_batch\n",
    "from utils.model import MobileNetV3_Household, load_model, save_model, print_model_summary\n",
    "from utils.visualization import plot_multiple_models_comparison\n",
    "from utils.compression import (\n",
    "    compare_experiments, compare_optimized_model_to_baseline, evaluate_optimized_model, list_experiments,  # For experimentation\n",
    "    is_quantized  # For quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore PyTorch deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Optional: Ignore all user warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "devices = [\"cpu\"]\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    devices.extend([f\"cuda:{i} ({torch.cuda.get_device_name(i)})\" for i in range(num_devices)])\n",
    "print(f\"Devices available: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_deterministic_mode(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = seed + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "    \n",
    "    return seed_worker\n",
    "\n",
    "set_deterministic_mode(42)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load household objects dataset\n",
    "train_loader, test_loader = get_household_loaders(\n",
    "    image_size=\"CIFAR\", batch_size=128, num_workers=2,\n",
    ")\n",
    "\n",
    "# Get input_size\n",
    "input_size = get_input_size(\"CIFAR\")\n",
    "print(f\"Input has size: {input_size}\")\n",
    "\n",
    "# Get class names\n",
    "class_names = train_loader.dataset.classes\n",
    "print(f\"Datasets have these classes: \")\n",
    "for i in range(len(class_names)):\n",
    "    print(f\"  {i}: {class_names[i]}\")\n",
    "\n",
    "# Visualize some examples\n",
    "for dataset_type, data_loader in [('train', train_loader), ('test', test_loader)]:\n",
    "    print(f\"\\nInformation on {dataset_type} set\")\n",
    "    print_dataloader_stats(data_loader, dataset_type)\n",
    "    print(f\"Examples of images from the {dataset_type} set\")\n",
    "    visualize_batch(data_loader, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the baseline model and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the baseline model\n",
    "baseline_model = MobileNetV3_Household()\n",
    "baseline_model_name = \"baseline_mobilenet\"\n",
    "baseline_model.load_state_dict(torch.load(f\"../models/{baseline_model_name}/checkpoints/model.pth\"))\n",
    "print_model_summary(baseline_model)\n",
    "\n",
    "# Load baseline metrics\n",
    "with open(f\"../results/{baseline_model_name}/metrics.json\", \"r\") as f:\n",
    "    baseline_metrics = json.load(f)\n",
    "\n",
    "print(\"\\nBaseline Model Metrics:\")\n",
    "pprint.pp(baseline_metrics)\n",
    "\n",
    "# Calculate target metrics based on CTO requirements\n",
    "target_model_size = baseline_metrics['size']['model_size_mb'] * (1 - TARGET_MODEL_COMPRESSION)\n",
    "target_inference_time_cpu = baseline_metrics['timing']['cpu']['avg_time_ms'] * (1 - TARGET_INFERENCE_SPEEDUP)\n",
    "if torch.cuda.is_available():\n",
    "    target_inference_time_gpu = baseline_metrics['timing']['cuda']['avg_time_ms'] * (1 - TARGET_INFERENCE_SPEEDUP)\n",
    "min_acceptable_accuracy = baseline_metrics['accuracy']['top1_acc'] * (1 - MAX_ALLOWED_ACCURACY_DROP) \n",
    "\n",
    "print(\"Optimization Targets:\")\n",
    "print(f\"Target Model Size: {baseline_metrics['size']['model_size_mb']:.2f} --> {target_model_size:.2f} MB ({TARGET_MODEL_COMPRESSION*100}% reduction)\")\n",
    "print(f\"Target Inference Time (CPU): {baseline_metrics['timing']['cpu']['avg_time_ms']:.2f} --> {target_inference_time_cpu:.2f} ms ({TARGET_INFERENCE_SPEEDUP*100}% reduction)\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Target Inference Time (GPU): {baseline_metrics['timing']['cuda']['avg_time_ms']:.2f} --> {target_inference_time_gpu:.2f} ms ({TARGET_INFERENCE_SPEEDUP*100}% reduction)\")\n",
    "print(f\"Minimum Acceptable Accuracy: {baseline_metrics['accuracy']['top1_acc']:.2f} --> {min_acceptable_accuracy:.2f} (within {MAX_ALLOWED_ACCURACY_DROP*100}% of baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement and evaluate optimization pipelines\n",
    "\n",
    "Based on your analysis in the previous notebook, you'll implement and evaluate different multi-step pipelines to find the optimal approach for meeting all requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Feel free to change the class entirely, or to move to a function if preferred\n",
    "class OptimizationPipeline:\n",
    "    def __init__(self, name, baseline_model, train_loader, test_loader, class_names, input_size):\n",
    "        \"\"\"\n",
    "        Initialize the optimization pipeline.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the pipeline for tracking and saving\n",
    "            baseline_model: The baseline model to optimize\n",
    "            train_loader: DataLoader for training data (needed for some optimization techniques)\n",
    "            test_loader: DataLoader for testing data (needed for evaluation)\n",
    "            class_names: List of class names in the dataset\n",
    "            input_size: Input tensor size\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.baseline_model = baseline_model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.class_names = class_names\n",
    "        self.input_size = input_size\n",
    "        self.optimized_model = None\n",
    "        self.steps = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for this pipeline\n",
    "        self.model_dir = f\"../models/pipeline/{name}\"\n",
    "        self.checkpoint_dir = f\"{self.model_dir}/checkpoints\"\n",
    "        self.results_dir = f\"../results/pipeline/{name}\"\n",
    "        \n",
    "        for d in [self.model_dir, self.checkpoint_dir, self.results_dir]:\n",
    "            os.makedirs(d, exist_ok=True)\n",
    "    \n",
    "    def add_step(self, step_name, step_function, **kwargs):\n",
    "        \"\"\"\n",
    "        Add an optimization step to the pipeline.\n",
    "        \n",
    "        Args:\n",
    "            step_name: Name of the step\n",
    "            step_function: Function that implements the step\n",
    "            **kwargs: Arguments to pass to the step function\n",
    "        \"\"\"\n",
    "        self.steps.append({\n",
    "            'name': step_name,\n",
    "            'function': step_function,\n",
    "            'args': kwargs\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def run(self, device=torch.device('cpu'), file_extension='pth'):\n",
    "        \"\"\"\n",
    "        Run the optimization pipeline.\n",
    "        \n",
    "        Args:\n",
    "            device: Device to run the pipeline on\n",
    "            file_extension: File extension to save the model with (.pt for torchscript, else .pth)\n",
    "            \n",
    "        Returns:\n",
    "            The optimized model\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Running pipeline: {self.name}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        # Start with the baseline model\n",
    "        current_model = self.baseline_model\n",
    "        \n",
    "        # Save intermediate results after each step\n",
    "        step_results = []\n",
    "        \n",
    "        # TODO: Run the pipeline iteratively\n",
    "        # For each stage in the pipeline:\n",
    "        # 1. Apply the specified technique with the given parameters\n",
    "        # 2. Evaluate the model after applying the technique\n",
    "        # 3. Store the results for later comparison\n",
    "        for i, step in enumerate(self.steps):\n",
    "            print(f\"\\n{'-'*50}\")\n",
    "            print(f\"Step {i+1}: {step['name']}\")\n",
    "            print(f\"{'-'*50}\\n\")\n",
    "            \n",
    "            # ...Add your code here...\n",
    "        \n",
    "        self.optimized_model = current_model\n",
    "        final_path = f\"{self.model_dir}/model.{file_extension}\"\n",
    "        \n",
    "        # TODO: Save final model\n",
    "        # Remember that different pytorch model format will require different saving mechanisms\n",
    "        \n",
    "        # Save pipeline results\n",
    "        self.results = {\n",
    "            'pipeline_name': self.name,\n",
    "            'steps': step_results,\n",
    "            'final_metrics': None,  # As returned by evaluate_optimized_model()\n",
    "            'final_comparison': None  # As returned by compare_optimized_model_to_baseline()\n",
    "        }\n",
    "        \n",
    "        with open(f\"{self.results_dir}/pipeline_metrics.json\", 'w') as f:\n",
    "            json.dump(self.results, f, indent=4)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Pipeline {self.name} completed\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        return self.optimized_model\n",
    "    \n",
    "    def visualize_results(self, baseline_metrics=baseline_metrics, device=torch.device('cpu')):\n",
    "        \"\"\"\n",
    "        Visualize the results of the pipeline.\n",
    "\n",
    "        Args:\n",
    "            baseline_metrics: Dictionary of baseline metrics for comparison.\n",
    "            device: Device to run the pipeline on\n",
    "\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to visualize. Please run the pipeline first.\")\n",
    "            return\n",
    "\n",
    "        # Define device name\n",
    "        device_name = 'cpu' if device==torch.device('cpu') else 'cuda'\n",
    "\n",
    "        # Extract metrics from each step\n",
    "        step_names = [step['step_name'] for step in self.results['steps']]\n",
    "        model_sizes = [step['metrics']['size']['model_size_mb'] for step in self.results['steps']]\n",
    "        model_memory_sizes = [step['metrics']['size']['total_params'] for step in self.results['steps']]\n",
    "        times = [step['metrics']['timing'][device_name]['avg_time_ms'] for step in self.results['steps']]\n",
    "        accuracies = [step['metrics']['accuracy']['top1_acc'] for step in self.results['steps']]\n",
    "\n",
    "        # Add baseline metrics\n",
    "        step_names.insert(0, 'Baseline')\n",
    "        baseline_size = baseline_metrics['size']['model_size_mb']\n",
    "        baseline_memory_size = baseline_metrics['size']['total_params']\n",
    "        baseline_inference_time = baseline_metrics['timing'][device_name]['avg_time_ms']\n",
    "        baseline_accuracy = baseline_metrics['accuracy']['top1_acc']\n",
    "\n",
    "        model_sizes.insert(0, baseline_size)\n",
    "        model_memory_sizes.insert(0, baseline_memory_size)\n",
    "        times.insert(0, baseline_inference_time)\n",
    "        accuracies.insert(0, baseline_accuracy)\n",
    "\n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(4, 1, figsize=(12, 15))\n",
    "\n",
    "        # Plot model size\n",
    "        axes[0].bar(step_names, model_sizes, color='blue')\n",
    "        axes[0].set_title('Model Size (MB)')\n",
    "        axes[0].set_ylabel('Size (MB)')\n",
    "        axes[0].axhline(y=baseline_size * (1-TARGET_MODEL_COMPRESSION), color='r', linestyle='--', label=f\"Target ({TARGET_MODEL_COMPRESSION*100}% reduction)\")\n",
    "        axes[0].legend()\n",
    "        for i, v in enumerate(model_sizes):\n",
    "            axes[0].text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "        axes[1].bar(step_names, model_memory_sizes, color='blue')\n",
    "        axes[1].set_title('Model Size (# Parameters)')\n",
    "        axes[1].set_ylabel('Peak Memory')\n",
    "        axes[1].axhline(y=baseline_memory_size * (1-TARGET_MODEL_COMPRESSION), color='r', linestyle='--', label=f\"Target ({TARGET_MODEL_COMPRESSION*100}% reduction)\")\n",
    "        axes[1].legend()\n",
    "        for i, v in enumerate(model_memory_sizes):\n",
    "            axes[1].text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "        # Plot inference time\n",
    "        axes[2].bar(step_names, times, color='green')\n",
    "        axes[2].set_title('Inference Time (ms)')\n",
    "        axes[2].set_ylabel('Time (ms)')\n",
    "        axes[2].axhline(y=baseline_inference_time * (1-TARGET_INFERENCE_SPEEDUP), color='r', linestyle='--', label=f\"Target ({TARGET_INFERENCE_SPEEDUP*100}% reduction)\")\n",
    "        axes[2].legend()\n",
    "        for i, v in enumerate(times):\n",
    "            axes[2].text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "        # Plot accuracy\n",
    "        axes[3].bar(step_names, accuracies, color='purple')\n",
    "        axes[3].set_title('Top-1 Accuracy (%)')\n",
    "        axes[3].set_ylabel('Accuracy (%)')\n",
    "        axes[3].axhline(y=baseline_accuracy * (1-MAX_ALLOWED_ACCURACY_DROP), color='r', linestyle='--', label=f\"Minimum acceptable ({MAX_ALLOWED_ACCURACY_DROP*100}% of baseline)\")\n",
    "        axes[3].legend()\n",
    "        for i, v in enumerate(accuracies):\n",
    "            axes[3].text(i, v + 0.5, f\"{v:.2f}%\", ha='center')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.results_dir}/pipeline_visualization.png\")\n",
    "        plt.show()\n",
    "\n",
    "        # Print final results summary\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Pipeline {self.name} Results Summary\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Size comparison\n",
    "        size_reduction = (baseline_size - model_sizes[-1]) / baseline_size * 100\n",
    "        print(f\"\\nModel Size (MB):\")\n",
    "        print(f\"  Baseline: {baseline_size:.2f} MB\")\n",
    "        print(f\"  Final: {model_sizes[-1]:.2f} MB\")\n",
    "        print(f\"  Reduction: {size_reduction:.2f}%\")\n",
    "        target_size = baseline_size * (1-TARGET_MODEL_COMPRESSION)\n",
    "        if model_sizes[-1] <= target_size:\n",
    "            print(f\"  ✅ Meets target ({TARGET_MODEL_COMPRESSION*100}% reduction)\")\n",
    "        else:\n",
    "            print(f\"  ❌ Does not meet target (Goal: {target_size:.2f} MB)\")\n",
    "\n",
    "        memory_size_reduction = (baseline_memory_size - model_memory_sizes[-1]) / baseline_memory_size * 100\n",
    "        print(f\"\\nModel Size (# Parameters):\")\n",
    "        print(f\"  Baseline: {baseline_memory_size:.2f} MB\")\n",
    "        print(f\"  Final: {model_memory_sizes[-1]:.2f} MB\")\n",
    "        print(f\"  Reduction: {memory_size_reduction:.2f}%\")\n",
    "        target_memory_size = baseline_memory_size * (1-TARGET_MODEL_COMPRESSION)\n",
    "        if model_memory_sizes[-1] <= target_memory_size:\n",
    "            print(f\"  ✅ Meets target ({TARGET_MODEL_COMPRESSION*100}% reduction)\")\n",
    "        else:\n",
    "            print(f\"  ❌ Does not meet target (Goal: {target_memory_size:.2f} MB)\")\n",
    "\n",
    "\n",
    "        # Inference time comparison\n",
    "        time_reduction = (baseline_inference_time - times[-1]) / baseline_inference_time * 100\n",
    "        print(f\"\\nInference Time (CPU):\")\n",
    "        print(f\"  Baseline: {baseline_inference_time:.2f} ms\")\n",
    "        print(f\"  Final: {times[-1]:.2f} ms\")\n",
    "        print(f\"  Reduction: {time_reduction:.2f}%\")\n",
    "        target_time = baseline_inference_time * (1-TARGET_INFERENCE_SPEEDUP)\n",
    "        if times[-1] <= target_time:\n",
    "            print(f\"  ✅ Meets target ({TARGET_INFERENCE_SPEEDUP*100}% reduction)\")\n",
    "        else:\n",
    "            print(f\"  ❌ Does not meet target (Goal: {target_time:.2f} ms)\")\n",
    "\n",
    "        # Accuracy comparison\n",
    "        accuracy_change = (accuracies[-1] - baseline_accuracy) / baseline_accuracy * 100\n",
    "        print(f\"\\nAccuracy:\")\n",
    "        print(f\"  Baseline: {baseline_accuracy:.2f}%\")\n",
    "        print(f\"  Final: {accuracies[-1]:.2f}%\")\n",
    "        print(f\"  Change: {accuracy_change:.2f}%\")\n",
    "        min_acceptable = baseline_accuracy * (1-MAX_ALLOWED_ACCURACY_DROP)\n",
    "        if accuracies[-1] >= min_acceptable:\n",
    "            print(f\"  ✅ Meets target (within {MAX_ALLOWED_ACCURACY_DROP*100}% of baseline)\")\n",
    "        else:\n",
    "            print(f\"  ❌ Does not meet target (Goal: ≥{min_acceptable:.2f}%)\")\n",
    "\n",
    "        # Overall assessment\n",
    "        print(f\"\\nOverall Assessment:\")\n",
    "        if model_sizes[-1] <= target_size and times[-1] <= target_time and accuracies[-1] >= min_acceptable:\n",
    "            print(f\"  ✅ Pipeline meets all requirements\")\n",
    "        else:\n",
    "            print(f\"  ❌ Pipeline does not meet all requirements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create helper functions for each compression technique you want to experiment with as a pipeline step\n",
    "# Remember to parametrize the functions appropriately\n",
    "def apply_post_training_pruning():\n",
    "    pass\n",
    "\n",
    "def apply_dynamic_quantization():\n",
    "    pass\n",
    "\n",
    "def apply_graph_optimization():\n",
    "    pass\n",
    "\n",
    "def apply_knowledge_distillation():\n",
    "    pass\n",
    "\n",
    "def apply_in_training_pruning():\n",
    "    pass\n",
    "\n",
    "def apply_in_training_quantization():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelines\n",
    "\n",
    "Note: You may want to recreate the cell below for new pipelines too, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run Pipeline #1\n",
    "\n",
    "# TODO: Choose a meaningful and unique pipeline name \n",
    "pipeline_name = \"\"  \n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline1 = OptimizationPipeline(\n",
    "    name=pipeline_name,\n",
    "    baseline_model=baseline_model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    class_names=class_names,\n",
    "    input_size=input_size\n",
    ")\n",
    "\n",
    "# TODO: Add optimization steps using the add_step() function\n",
    "\n",
    "# Run the pipeline\n",
    "optimized_model_p1 = pipeline1.run()\n",
    "\n",
    "# Visualize the results\n",
    "# TODO: Choose the device to report performance for\n",
    "device = None  # Define as torch.device()\n",
    "pipeline1.visualize_results(baseline_metrics, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Compare All Pipelines\n",
    "\n",
    "Let's compare the results of all pipelines to determine which one best meets our requirements.\n",
    "\n",
    "Note that, for simplicity, we re-use the `compare_experiments()` function which will return each step of each pipeline and not final pipeline results all-together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all implemented pipelines\n",
    "pipeline_experiments = [os.path.join('pipeline', exp_name) for exp_name in list_experiments(\"../results/pipeline/\")]\n",
    "\n",
    "if pipeline_experiments:\n",
    "    _ = compare_experiments(pipeline_experiments, baseline_metrics=baseline_metrics)\n",
    "else:\n",
    "    print(\"No pipeline results available yet. Please run at least one pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "**TODO: Analyse the multi-step optimization results and collect lessons learnt from the optimization process**\n",
    "\n",
    "Based on your implementation of the multi-stage optimization pipeline, analyze how the combined techniques perform against the CTO's requirements.\n",
    "\n",
    "Consider these guiding questions:\n",
    "- How does your optimized model compare to the baseline across all metrics?\n",
    "- What contribution did each stage make to the final performance?\n",
    "- What technical insights did you gain about optimizing MobileNetV3?\n",
    "- What trade-offs emerged, and how did you balance competing priorities?\n",
    "- What further improvements might be possible?\n",
    "\n",
    "Provide a comprehensive analysis that demonstrates your understanding of the optimization process and its outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Stage Optimization Pipeline Analysis for UdaciSense Computer Vision Model  \n",
    "\n",
    "*(Replace this with your analysis)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🚀 **Next Step:** \n",
    "> Deploy the final model, optimized via the multi-step pipeline, in notebook `04_deployment.ipynb`  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
