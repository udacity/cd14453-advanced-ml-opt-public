{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624090f8",
   "metadata": {},
   "source": [
    "# Exercise 1: Profile a neural network for compression opportunities\n",
    "\n",
    "Before we can compress a model effectively, we need to understand its characteristics. Think of this as a \"_pre-compression checkup_\"—just like a doctor examines a patient before treatment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ad9024",
   "metadata": {},
   "source": [
    "> **Task**: Profile a pre-trained neural network to identify potential optimization opportunities by analyzing parameter distributions, layer sizes, and inference performance.\n",
    "> \n",
    "> **Goal**: By analyzing model characteristics like parameter distributions, layer sizes, and inference behavior, you'll gain insights that inform which compression techniques might be most effective.\n",
    "> \n",
    "> **Tools**: pytorch, numpy, pandas, matplotlib (seaborn)\n",
    "> <br> _Prior experience recommended!_\n",
    "> \n",
    "> **Estimated Time**: ~=15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6308feb",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f4745",
   "metadata": {},
   "source": [
    "> **Primer: Profiling tools**  \n",
    "> Throughout this exercise, we'll use basic Python timing functions and PyTorch utilities. In real-world scenarios, you might use:\n",
    "> - **PyTorch Profiler**: For detailed GPU kernel analysis\n",
    "> - **NVIDIA Nsight**: For hardware-level GPU profiling\n",
    "> - **Intel VTune**: For CPU profiling\n",
    "> - **Memory_profiler**: For Python memory profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ed1e2",
   "metadata": {},
   "source": [
    "## Step 2: Load and analyze model parameters\n",
    "\n",
    "We'll use [ResNet50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html), a widely-used convolutional neural network for image classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88229604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model: ResNet50\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce00ca9",
   "metadata": {},
   "source": [
    "## Step 3: Profile the pre-trained model\n",
    "\n",
    "Let's profile this model from multiple angles to understand its characteristics and potential optimization opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c00fd",
   "metadata": {},
   "source": [
    "### A. Calculate model size and parameter count\n",
    "\n",
    "First, we need to understand the overall size of the model to determine if it's suitable for our deployment targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e6c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate total parameters in the model\n",
    "# Hint: Not sure how to inspect model layers or count parameters?\n",
    "# See: https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "# and: https://discuss.pytorch.org/t/how-do-i-count-the-number-of-parameters-in-a-model/4325\n",
    "total_params = # Add your code here\n",
    "\n",
    "# TODO: Calculate trainable parameters\n",
    "# Hint: Parameters have an attribute indicating if they require gradients\n",
    "trainable_params = # Add your code here\n",
    "\n",
    "# TODO: Calculate model size in MB (assuming 32-bit float - 4 bytes per parameter)\n",
    "# Hint: Convert from bytes to megabytes using the appropriate divisor\n",
    "model_size_mb = # Add your code here\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size (MB): {model_size_mb:.2f}\")\n",
    "\n",
    "# Compare to common deployment constraints\n",
    "print(\"\\nDeployment Context:\")\n",
    "print(f\"- Typical mobile app size limit: ~100MB\")\n",
    "print(f\"- Many edge devices have <1GB of memory\")\n",
    "print(f\"- Browser-based models often target <10MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2604e86e",
   "metadata": {},
   "source": [
    "> **Before you continue: Think about this!**  \n",
    "> What percentage of a typical mobile app's size budget would this model consume?  \n",
    "> Would this model be feasible for browser-based deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d01922",
   "metadata": {},
   "source": [
    "### B. Analyze layer-by-layer distribution\n",
    "\n",
    "Understanding which layers contain the most parameters helps us target optimization efforts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4abd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps us extract detailed information about each layer in the model\n",
    "def collect_layer_stats(model):\n",
    "    stats = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        # TODO: Define which layers to analyze\n",
    "        # Hint: This is a list of layers that contain trainable parameters (weights and biases).\n",
    "        # You can review which layers are in the ResNet model using `dict(model.named_modules())` and define them in this list using `torch.nn``\n",
    "        # See: https://docs.pytorch.org/docs/stable/nn.html\n",
    "        layer_types = # Add your code here\n",
    "        if any([isinstance(module, l) for l in layer_types]):\n",
    "            \n",
    "            # TODO: Calculate total parameters in the module\n",
    "            # Hint: You can reuse the same logic as in Step 3., except you want to consider both parameters that are trainable and not\n",
    "            param_count = # Add your code here\n",
    "            # TODO: Calculate module size in MB (assuming 32-bit float - 4 bytes per parameter)\n",
    "            # Hint: You can reuse the same logic as in Step 3.\n",
    "            param_size_mb = # Add your code here\n",
    "            \n",
    "            # TODO: Extract parameter values for distribution analysis\n",
    "            # Hint: Should you select from both weight and bias?\n",
    "            params = # Add your (multi-line) code here\n",
    "                \n",
    "            stats.append({\n",
    "                'name': name,\n",
    "                'type': module.__class__.__name__,\n",
    "                'parameters': param_count,\n",
    "                'size_mb': param_size_mb,\n",
    "                'params_array': params\n",
    "            })\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Run the collection function\n",
    "layer_stats = collect_layer_stats(model)\n",
    "\n",
    "# Convert the stats list to a DataFrame\n",
    "stats_df = pd.DataFrame(layer_stats)\n",
    "\n",
    "# Calculate the percentage of total parameters for each layer\n",
    "stats_df['parameters_pct'] = stats_df['parameters'] / total_params * 100\n",
    "\n",
    "# Sort layers by parameter count (descending)\n",
    "stats_df_sorted = stats_df.sort_values('parameters', ascending=False)\n",
    "\n",
    "# Display top n layers by parameter count\n",
    "# TODO: Decide how many layers to analyze\n",
    "# Hint: This is an integer (e.g., 5, 10, 15, ...)\n",
    "n = # Add your code here\n",
    "print(f\"Top {n} layers by parameter count:\")\n",
    "display(stats_df_sorted[['name', 'type', 'parameters', 'size_mb', 'parameters_pct']].head(n))\n",
    "\n",
    "# Calculate parameters by layer type (group by)\n",
    "layer_type_stats = stats_df.groupby('type').agg({\n",
    "    'parameters': 'sum',\n",
    "    'size_mb': 'sum'\n",
    "}).reset_index()\n",
    "layer_type_stats['parameters_pct'] = layer_type_stats['parameters'] / total_params * 100\n",
    "layer_type_stats = layer_type_stats.sort_values('parameters', ascending=False)\n",
    "\n",
    "print(\"\\nParameters by layer type:\")\n",
    "display(layer_type_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819b0fd",
   "metadata": {},
   "source": [
    "> **Common pitfall**: Don't focus only on the largest layer! While large layers are obvious targets, the collective impact of many smaller layers can also be significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3253acab",
   "metadata": {},
   "source": [
    "### C. Visualize parameter distribution\n",
    "\n",
    "Visualization helps us intuitively understand model's characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart of parameters by layer type\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie(layer_type_stats['parameters_pct'], labels=layer_type_stats['type'], autopct='%1.1f%%', shadow=True)\n",
    "plt.title('Distribution of Parameters by Layer Type')\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a bar chart of top 5 layers by parameter count\n",
    "# Hint: Consider dividing parameters by 1,000,000 to show values in millions - it's more readable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51faeb3b",
   "metadata": {},
   "source": [
    "> **Business impact**  \n",
    "> The visualizations you've just created aren't just for technical analysis - they're powerful communication tools. When presenting to stakeholders:\n",
    "> \n",
    "> - **Pie charts** help non-technical audiences quickly grasp where model complexity is concentrated\n",
    "> - **Parameter distribution plots** can justify optimization decisions with clear visual evidence\n",
    "> - **Latency measurements** translated to business metrics (e.g., \"this allows us to serve 2x more customers\") are more compelling than technical metrics alone\n",
    "> \n",
    "> Always tailor your visualizations to your audience and connect them to business objectives!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1095099",
   "metadata": {},
   "source": [
    "### D. Analyze parameter distributions\n",
    "\n",
    "Looking at weight distributions can reveal if a layer has many redundant (near-zero) parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ffc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function analyzes the actual weight values to identify patterns like near-zero weights\n",
    "def plot_param_distribution(params, title, bins=50):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create histogram with density curve\n",
    "    sns.histplot(params, bins=bins, kde=True)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean = np.mean(params)\n",
    "    std = np.std(params)\n",
    "    median = np.median(params)\n",
    "    \n",
    "    # Add statistics text box\n",
    "    stats_text = f'Mean: {mean:.6f}\\nStd: {std:.6f}\\nMedian: {median:.6f}'\n",
    "    plt.annotate(stats_text, xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8),\n",
    "                 va='top')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Parameter Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # TODO: Return percentage of weights close to zero \n",
    "    # Hint: Use 0.01 as a first threshold, but consider experimenting with others too - how does it affect the results?\n",
    "    near_zero_pct = # Add your code here\n",
    "    return near_zero_pct\n",
    "\n",
    "# Find index of the largest convolutional layer by parameter count\n",
    "large_conv_layer = stats_df[stats_df['type'] == 'Conv2d'].sort_values('parameters', ascending=False).iloc[0]\n",
    "\n",
    "# TODO: Find index of the fully connected layer (Linear)\n",
    "fc_layer = # Add your code here\n",
    "\n",
    "# Use the plot_param_distribution function to visualize these layers\n",
    "# and store the percentage of near-zero weights\n",
    "near_zero_large_conv = plot_param_distribution(large_conv_layer['params_array'], \n",
    "                                             f\"Weight Distribution: {large_conv_layer['name']} (Large Conv)\")\n",
    "near_zero_fc = plot_param_distribution(fc_layer['params_array'], \n",
    "                                     f\"Weight Distribution: {fc_layer['name']} (FC Layer)\") if fc_layer is not None else 0\n",
    "\n",
    "print(f\"Percentage of near-zero weights in largest conv layer: {near_zero_large_conv:.2f}%\")\n",
    "print(f\"Percentage of near-zero weights in FC layer: {near_zero_fc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7558c7",
   "metadata": {},
   "source": [
    "> **Brainstorming check!**  \n",
    "> What does a high percentage of near-zero weights suggest about a layer?  \n",
    "> How might this insight guide optimization approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16c0e8",
   "metadata": {},
   "source": [
    "### E. Profile inference latency\n",
    "\n",
    "Measuring inference time helps identify if the model meets real-time requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to measure inference time\n",
    "def measure_inference_time(model, input_tensor, num_runs=50):\n",
    "    # TODO: Add warmup runs\n",
    "    # Hint: You can use a loop to perform a few runs (e.g., 5 or 10) without recording the timing. This can be added before you start measuring latency.\n",
    "    \n",
    "    # Measure inference time for multiple runs\n",
    "    latencies = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            _ = model(input_tensor)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            latencies.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "    \n",
    "    # Calculate statistics\n",
    "    # TODO: Choose an appropriate percentile threshold for latency analysis\n",
    "    # Hint: Output is a value between 0 and 100. Common choices are P95 (95th percentile) and P99 (99th percentile)\n",
    "    p_value = # Add your code here\n",
    "    avg_latency = np.mean(latencies)\n",
    "    p_latency = np.percentile(latencies, p_value)\n",
    "    \n",
    "    return avg_latency, (p_latency, p_value)\n",
    "\n",
    "\n",
    "# Create input data for inference\n",
    "# We need representative input data to measure realistic inference performance\n",
    "batch_size = 1\n",
    "input_tensor = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "\n",
    "# TODO: Choose which device to run profiling on based on your preferred target deployment scenario (edge, cloud, ...)\n",
    "# Hint: Use torch.device() with the desired hardware type\n",
    "device_for_profiling = # Add your code here\n",
    "model_for_profiling = model.to(device_for_profiling)\n",
    "input_for_profiling = input_tensor.to(device_for_profiling)\n",
    "\n",
    "# Run the profiling\n",
    "avg_latency, (p_latency, p_value) = measure_inference_time(model_for_profiling, input_for_profiling)\n",
    "\n",
    "print(f\"Average inference latency: {avg_latency:.2f} ms\")\n",
    "print(f\"P{p_value} inference latency: {p_latency:.2f} ms\")\n",
    "\n",
    "# Context for these numbers\n",
    "print(\"\\nPerformance Context:\")\n",
    "print(\"- Human perception threshold for 'instant': ~100ms\")\n",
    "print(\"- Interactive applications typically target <50ms latency\")\n",
    "print(\"- Real-time video (30 FPS) requires processing in <33ms per frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92560714",
   "metadata": {},
   "source": [
    "> **Profiling tip**  \n",
    "> Always include warmup runs when profiling neural networks. The first few runs are often slower due to:\n",
    "> - JIT compilation (especially with PyTorch)\n",
    "> - GPU kernel caching\n",
    "> - Memory allocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65d1ca",
   "metadata": {},
   "source": [
    "## Step 4. Summarize profiling results\n",
    "\n",
    "Now we bring together all our findings to identify the most promising optimization opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the top 3 layers by parameter count from your analysis\n",
    "largest_layers = [\n",
    "    stats_df_sorted.iloc[0]['name'],\n",
    "    stats_df_sorted.iloc[1]['name'],\n",
    "    stats_df_sorted.iloc[2]['name']\n",
    "]\n",
    "\n",
    "# TODO: Estimate the percentage of potentially redundant parameters\n",
    "# Hint: Combine the near_zero_large_conv and near_zero_fc values calculated in step D\n",
    "redundant_params_pct = # Add your code here\n",
    "\n",
    "# Summarize performance characteristics\n",
    "model_summary = {\n",
    "    \"total_parameters\": total_params,\n",
    "    \"model_size_mb\": model_size_mb,\n",
    "    \"largest_layer_type\": layer_type_stats.iloc[0]['type'],\n",
    "    \"largest_layers\": largest_layers,\n",
    "    \"redundant_params_pct\": redundant_params_pct,\n",
    "    \"avg_inference_latency_ms\": avg_latency,\n",
    "    f\"p{p_value}_inference_latency_ms\": p_latency,\n",
    "}\n",
    "\n",
    "# Print a formatted summary\n",
    "print(\"=\"*50)\n",
    "print(\"MODEL PROFILING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for key, value in model_summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e113f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you've learned how to:\n",
    "- Analyze the parameter distribution across a neural network\n",
    "- Identify which layers consume the most memory\n",
    "- Examine parameter value distributions within layers\n",
    "- Perform basic inference latency profiling\n",
    "\n",
    "These skills form the foundation for effective model optimization. Before you can make a model smaller or faster, you need to understand where the size and performance bottlenecks are."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
