{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a4016d8",
   "metadata": {},
   "source": [
    "# Exercise 2: Implement structured pruning on a small language model with Hugging Face Transformers\n",
    "\n",
    "Neural network pruning is a powerful technique to reduce model size and improve inference performance. In this exercise, you'll apply structured pruning to a pre-trained language model using the Hugging Face Transformers library.\n",
    "\n",
    "> **Task**: Implement structured attention head pruning on a pre-trained DistilBERT model to remove 30% of the least important heads while monitoring the accuracy-efficiency tradeoff.\n",
    "> \n",
    "> **Goal**: By the end of this exercise, you'll understand how to identify redundant attention heads, implement structured pruning without specialized libraries, and evaluate the resulting model's performance.\n",
    "> \n",
    "> **Scenario**: You're developing a customer feedback analysis system for a retail chain. The system processes product reviews and social media mentions to extract sentiment in real-time across thousands of stores. Your current DistilBERT model (fine-tuned on SST-2) performs excellently but consumes too much memory and compute for your distributed edge servers.\n",
    "> <br> The initial model works great on movie reviews (SST-2's domain) and transfers well to product reviews. However, deploying it to every store location would require expensive hardware upgrades. Since the model already achieves 91% accuracy—more than sufficient for business insights—you've chosen structured pruning to trade a small amount of accuracy for significant efficiency gains.\n",
    "> \n",
    "> **Tools**: pytorch, transformers, numpy, pandas, matplotlib\n",
    "> <br> Prior experience recommended!\n",
    "> \n",
    "> **Estimated Time**: ~=15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca720b8",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3960297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out the cell block again and restart the notebook\n",
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TextClassificationPipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set the device for pytorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffc893",
   "metadata": {},
   "source": [
    "## Step 2: Load a pre-trained model and dataset\n",
    "\n",
    "We'll use a small BERT model (`distilbert-base-uncased`) fine-tuned for sentiment analysis. This model is relatively compact but still demonstrates the benefits of pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Move to GPU if available\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load a small test dataset (SST-2)\n",
    "test_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation[:500]\")\n",
    "print(f\"Test dataset loaded: {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f8c3a",
   "metadata": {},
   "source": [
    "> **Why DistilBERT with the Stanford Sentiment Treebank (SST-2) dataset?**: We're using DistilBERT for this exercise because:\n",
    "> \n",
    "> - It's already a compressed version of BERT (using knowledge distillation)\n",
    "> - It has a simpler architecture than full BERT but retains the key transformer components\n",
    "> - It's pre-trained and fine-tuned for sentiment analysis on SST-2, making evaluation straightforward\n",
    "> \n",
    "> The Stanford Sentiment Treebank (SST-2) dataset contains movie review sentences with binary sentiment labels (positive/negative), making it perfect for evaluating our pruned model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a469cc0",
   "metadata": {},
   "source": [
    "# Step 3: Analyze the model's architecture\n",
    "\n",
    "Let's first understand the structure of our model and calculate basic metrics like parameter count and model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate total parameters in the model\n",
    "# Hint: Not sure how to inspect model layers or count parameters?\n",
    "# See: https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "# and https://discuss.pytorch.org/t/how-do-i-count-the-number-of-parameters-in-a-model/4325\n",
    "total_params = # Add your code here \n",
    "\n",
    "# TODO: Calculate trainable parameters\n",
    "# Hint: Parameters have an attribute indicating if they require gradients\n",
    "trainable_params = # Add your code here\n",
    "\n",
    "# TODO: Calculate model size in MB (assuming 32-bit float - 4 bytes per parameter)\n",
    "# Hint: Convert from bytes to megabytes using the appropriate divisor\n",
    "model_size_mb = # Add your code here\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size (MB): {model_size_mb:.2f}\")\n",
    "\n",
    "# Count attention heads\n",
    "def count_attention_heads(model):\n",
    "    \"\"\"Count total attention heads in a transformer model.\"\"\"\n",
    "    total_heads = 0\n",
    "    for module in model.modules():\n",
    "        # TODO: Define if a module is an attention head\n",
    "        # Hint: You can check modules' attributes at runtime using `module.__dict__` or `print(module)`\n",
    "        is_attention_module = # Add your code here\n",
    "        # Check for attention modules in the model\n",
    "        if is_attention_module:\n",
    "            total_heads += module.n_heads\n",
    "    return total_heads\n",
    "\n",
    "print(f\"Total attention heads in the model: {count_attention_heads(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c0e08",
   "metadata": {},
   "source": [
    "> **Understanding the model architecture**: DistilBERT has a simpler architecture than BERT:\n",
    "> \n",
    "> 6 transformer layers (vs. 12 in BERT-base)\n",
    "> 12 attention heads per layer (same as BERT-base)\n",
    "> 768 hidden dimensions (same as BERT-base)\n",
    "> \n",
    "> Each attention head processes information differently, capturing various linguistic patterns and relationships. However, not all heads contribute equally to the model's performance. By identifying and removing the least important heads, we can reduce model size and inference time while maintaining most of the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45635469",
   "metadata": {},
   "source": [
    "# Step 4: Create evaluation pipeline\n",
    "\n",
    "Before we start pruning, we need a way to evaluate our model's performance. Let's create a function that measures accuracy, latency, and model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc457b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, device, num_samples=100, batch_size=16):\n",
    "    \"\"\"Evaluate model accuracy and latency on a dataset.\"\"\"\n",
    "    # Move model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = TextClassificationPipeline(\n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        device=device if device.type != \"cpu\" else -1,\n",
    "        return_all_scores=True\n",
    "    )\n",
    "    \n",
    "    # Prepare inputs\n",
    "    texts = dataset[\"sentence\"][:num_samples]\n",
    "    labels = dataset[\"label\"][:num_samples]\n",
    "    \n",
    "    # Measure accuracy\n",
    "    correct = 0\n",
    "    latencies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            \n",
    "            # Measure inference time\n",
    "            start_time = time.time()\n",
    "            outputs = pipeline(batch_texts)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            latency = (end_time - start_time) * 1000  # Convert to ms\n",
    "            latencies.append(latency)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            for j, output in enumerate(outputs):\n",
    "                pred_label = 0 if output[0][\"score\"] > output[1][\"score\"] else 1\n",
    "                if pred_label == batch_labels[j]:\n",
    "                    correct += 1\n",
    "    \n",
    "    accuracy = correct / num_samples\n",
    "    avg_latency = np.mean(latencies) / batch_size  # Latency per example\n",
    "    \n",
    "    # TODO: Calculate model size in MB\n",
    "    # Hint: You need to calculate the model size, but then you can reuse the same logic as in Step 3 to move to MB\n",
    "    model_size = # Add your code here\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"avg_latency_ms\": avg_latency,\n",
    "        \"model_size_mb\": model_size\n",
    "    }\n",
    "\n",
    "# Evaluate original model\n",
    "print(\"Evaluating original model...\")\n",
    "original_metrics = evaluate_model(model, tokenizer, test_dataset, device)\n",
    "\n",
    "print(f\"Original model metrics:\")\n",
    "print(f\"- Accuracy: {original_metrics['accuracy']:.4f}\")\n",
    "print(f\"- Avg Latency: {original_metrics['avg_latency_ms']:.2f} ms\")\n",
    "print(f\"- Model Size: {original_metrics['model_size_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f4467",
   "metadata": {},
   "source": [
    "> **Brainstorming check!**\n",
    "> <br>\n",
    "> What other metrics would you track?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77504822",
   "metadata": {},
   "source": [
    "## Step 5: Implement structured attention head pruning\n",
    "\n",
    "Now we'll implement the core of our exercise: structured pruning of attention heads in a transformer model. This is a multi-step process that involves:\n",
    "\n",
    "1. Identifying which attention heads are least important\n",
    "2. Creating a mechanism to measure head importance\n",
    "3. Selecting heads for pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c2a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze attention head importance\n",
    "def analyze_head_importance(model, tokenizer, dataset, device, num_samples=50):\n",
    "    \"\"\"Analyze the importance of each attention head.\"\"\"\n",
    "    head_importance = {}\n",
    "    model.eval()\n",
    "    \n",
    "    # Get attention modules\n",
    "    attention_modules = []\n",
    "    for name, module in model.named_modules():\n",
    "        # TODO: Define if a module is an attention head\n",
    "        # Hint: You can reuse the same logic as in Step 3.\n",
    "        is_attention_module = # Add your code here\n",
    "        if is_attention_module:\n",
    "            attention_modules.append((name, module))\n",
    "    \n",
    "    # Sample some data\n",
    "    inputs = tokenizer(dataset[\"sentence\"][:num_samples], \n",
    "                       padding=True, \n",
    "                       truncation=True, \n",
    "                       return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get baseline outputs\n",
    "    with torch.no_grad():\n",
    "        baseline_outputs = model(**inputs).logits\n",
    "        baseline_probs = torch.softmax(baseline_outputs, dim=1)\n",
    "    \n",
    "    # For each attention module and head\n",
    "    for module_name, module in attention_modules:\n",
    "        head_importance[module_name] = []\n",
    "        \n",
    "        # For each head in this module\n",
    "        for head_idx in range(module.n_heads):\n",
    "            # Mask a specific attention head and measure its impact\n",
    "            def mask_head(module, inputs, head_idx=head_idx):\n",
    "                # Store the original attention output\n",
    "                original_func = module.forward\n",
    "                \n",
    "                # Define a new forward function that masks a specific head\n",
    "                def masked_forward(*args, **kwargs):\n",
    "                    outputs = original_func(*args, **kwargs)\n",
    "                    # Mask the attention scores for this head\n",
    "                    if isinstance(outputs, tuple):\n",
    "                        attn_output = outputs[0]\n",
    "                        head_size = attn_output.size(-1) // module.n_heads\n",
    "                        # TODO: Create indices to zero out for this specific head\n",
    "                        # Hint: Each head occupies a contiguous block of size `head_size` in the last dimension.\n",
    "                        # Calculate the start and end index for the block belonging to `head_idx`, and pass it to torch.arange() with the right device\n",
    "                        indices = # Add your code here\n",
    "                        attn_output[:, :, indices] = 0\n",
    "                        return outputs\n",
    "                    return outputs\n",
    "                \n",
    "                # Replace the forward method\n",
    "                module.forward = masked_forward\n",
    "                \n",
    "                # Run the model\n",
    "                with torch.no_grad():\n",
    "                    masked_outputs = model(**inputs).logits\n",
    "                    masked_probs = torch.softmax(masked_outputs, dim=1)\n",
    "                \n",
    "                # Restore the original forward method\n",
    "                module.forward = original_func\n",
    "                \n",
    "                return masked_probs\n",
    "            \n",
    "            # Run the model with this head masked\n",
    "            masked_probs = mask_head(module, inputs)\n",
    "            \n",
    "            # TODO: Calculate head importance as the difference in output when the head is masked\n",
    "            # Hint: Consider how to measure the difference between two probability distributions\n",
    "            prob_diff = # Add your code here\n",
    "            head_importance[module_name].append(prob_diff)\n",
    "    \n",
    "    print(f\"Analysis completed.\")\n",
    "    return head_importance\n",
    "\n",
    "# Analyze head importance\n",
    "print(\"Analyzing attention head importance...\")\n",
    "head_importance = analyze_head_importance(model, tokenizer, test_dataset, device)\n",
    "pprint(head_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a449a8",
   "metadata": {},
   "source": [
    "> **How does head importance work?**: The technique we're using is called masking-based importance scoring. Here's how it works:\n",
    "> \n",
    "> 1. We run the model normally to get baseline predictions\n",
    "> 2. For each attention head, we temporarily modify the model to mask (zero out) that head's output\n",
    "> 3. We compare the new predictions with the baseline\n",
    "> 4. The larger the difference, the more important the head is\n",
    "> \n",
    "> This is a form of ablation analysis - we're measuring the impact of removing a component. Heads with minimal impact on the output are good candidates for pruning.\n",
    "> \n",
    "> The method is computationally efficient because we only need to run inference, not training or fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed0476",
   "metadata": {},
   "source": [
    "## Step 6: Visualize head importance and select heads for pruning\n",
    "\n",
    "Now that we've computed importance scores for each attention head, let's visualize the results and select which heads to prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize head importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "for module_name, importances in head_importance.items():\n",
    "    module_name = module_name.split('.')[-1]  # Shorten the name\n",
    "    plt.bar(\n",
    "        [f\"{module_name}-{i}\" for i in range(len(importances))],\n",
    "        importances\n",
    "    )\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Attention Head Importance\")\n",
    "plt.xlabel(\"Attention Heads\")\n",
    "plt.ylabel(\"Importance Score (higher = more important)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Determine how many heads to prune based on a pruning rate\n",
    "# Hint: Consider what percentage of heads you want to remove\n",
    "pruning_rate = # Add your code here\n",
    "\n",
    "# TODO: Calculate the number of heads to prune based on the pruning rate\n",
    "# Hint: Count the total number of heads and apply the rate\n",
    "n_heads_to_prune = # Add your code here\n",
    "\n",
    "# Flatten and sort all heads by importance\n",
    "all_heads = []\n",
    "for module_name, importances in head_importance.items():\n",
    "    for i, imp in enumerate(importances):\n",
    "        all_heads.append((module_name, i, imp))\n",
    "\n",
    "# TODO: Sort heads by importance to identify which to prune first\n",
    "# Hint: Each item is a tuple of (module_name, head_idx, importance). Also, should you prune in ascending or descending order?\n",
    "# Finally, feel free to sort in place instead!\n",
    "all_heads = # Add your code here\n",
    "\n",
    "# Get the least important heads\n",
    "heads_to_prune = all_heads[:n_heads_to_prune]\n",
    "print(f\"Pruning {len(heads_to_prune)} heads with lowest importance scores\")\n",
    "\n",
    "# Print the heads to be pruned\n",
    "for module_name, head_idx, imp in heads_to_prune:\n",
    "    print(f\"Pruning {module_name} head {head_idx} (importance: {imp:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b45cd",
   "metadata": {},
   "source": [
    "> **Interpreting the visualization**: The bar chart shows the importance score for each attention head across all layers. You should notice:\n",
    "> \n",
    "> 1. Varying Importance: Some heads have much higher scores than others\n",
    "> 2. Layer Patterns: Heads in certain layers may be systematically more important\n",
    "> 3. Redundancy: Many heads have very low importance scores\n",
    "> \n",
    "> This uneven distribution confirms our hypothesis that not all attention heads contribute equally to the model's performance. By pruning the least important heads (those with the lowest bars), we can reduce model complexity while minimizing impact on accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e9a7b",
   "metadata": {},
   "source": [
    "# Step 7: Implement head pruning\n",
    "\n",
    "Now that we've identified which attention heads to prune, let's implement the actual pruning operation. This involves modifying the model architecture to remove the selected heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a110f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_heads_in_model(model, heads_to_prune):\n",
    "    \"\"\"Prune attention heads in the model using Hugging Face's built-in method.\"\"\"\n",
    "    # Format the heads to prune into a dictionary that matches HuggingFace's format\n",
    "    # {layer_num: [list of heads to prune in this layer]}\n",
    "    pruning_dict = {}\n",
    "    \n",
    "    # Create a dictionary of heads to prune by layer index\n",
    "    for module_name, head_idx, _ in heads_to_prune:\n",
    "        # Extract layer index from module name (e.g., 'transformer.layer.0.attention')\n",
    "        layer_parts = module_name.split('.')\n",
    "        for i, part in enumerate(layer_parts):\n",
    "            if part == 'layer' and i+1 < len(layer_parts) and layer_parts[i+1].isdigit():\n",
    "                layer_idx = int(layer_parts[i+1])\n",
    "                \n",
    "                # TODO: Update the pruning dictionary for this layer for pruning\n",
    "                # Hint: The expected output is a list. What would an empty list represent here?\n",
    "                if layer_idx not in pruning_dict:\n",
    "                    pruning_dict[layer_idx] = # Add your code here\n",
    "                pruning_dict[layer_idx].append(head_idx)\n",
    "                break\n",
    "    \n",
    "    # Print the pruning plan\n",
    "    print(\"Pruning plan:\")\n",
    "    for layer_idx, heads in pruning_dict.items():\n",
    "        print(f\"Layer {layer_idx}: Pruning heads {heads}\")\n",
    "    \n",
    "    # Clone the model to avoid modifying the original\n",
    "    pruned_model = type(model)(model.config)\n",
    "    pruned_model.load_state_dict(model.state_dict())\n",
    "    pruned_model = pruned_model.to(device)\n",
    "    \n",
    "    # Perform pruning layer by layer\n",
    "    for layer_idx, heads in pruning_dict.items():\n",
    "        # Find the attention module for this layer\n",
    "        attention_module = None\n",
    "        for name, module in pruned_model.named_modules():\n",
    "            if f'layer.{layer_idx}.attention' in name and hasattr(module, 'prune_heads'):\n",
    "                attention_module = module\n",
    "                break\n",
    "        \n",
    "        if attention_module is not None:\n",
    "            # TODO: Prune the defined attention heads for the module\n",
    "            # Hint: Hugging Face provides a built-in functionality that updated attention_module in place!\n",
    "            # See: https://huggingface.co/docs/transformers/main/main_classes/model\n",
    "            # Add your one-line code here\n",
    "        else:\n",
    "            print(f\"Warning: Could not find attention module for layer {layer_idx}\")\n",
    "    \n",
    "    return pruned_model\n",
    "\n",
    "# Prune the model\n",
    "pruned_model = prune_heads_in_model(model, heads_to_prune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b6021",
   "metadata": {},
   "source": [
    "> **Understanding the structured pruning implementation**: This implementation of structured pruning has several key components:\n",
    "> \n",
    "> - Weight Masking: We create boolean masks to select which weights to keep and which to discard\n",
    "> - Architecture Modification: We actually change the structure of the model by removing rows/columns from weight matrices\n",
    "> - Metadata Updates: We update the model's metadata (like number of heads) to reflect the new architecture\n",
    "> \n",
    "> Unlike unstructured pruning which simply zeros out weights, structured pruning creates a truly smaller model with fewer parameters. This results in actual memory savings and computational speedups on standard hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a330ef",
   "metadata": {},
   "source": [
    "# Step 8: Evaluate and compare models\n",
    "\n",
    "Now let's apply our pruning function and evaluate the pruned model's performance compared to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf44b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate pruned model\n",
    "print(\"Evaluating pruned model...\")\n",
    "pruned_metrics = evaluate_model(pruned_model, tokenizer, test_dataset, device)\n",
    "\n",
    "print(f\"Pruned model metrics:\")\n",
    "print(f\"- Accuracy: {pruned_metrics['accuracy']:.4f}\")\n",
    "print(f\"- Avg Latency: {pruned_metrics['avg_latency_ms']:.2f} ms\")\n",
    "print(f\"- Model Size: {pruned_metrics['model_size_mb']:.2f} MB\")\n",
    "\n",
    "# Compare before and after pruning\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Accuracy change: {(pruned_metrics['accuracy'] - original_metrics['accuracy']) * 100:.2f}%\")\n",
    "print(f\"Latency improvement: {(1 - pruned_metrics['avg_latency_ms'] / original_metrics['avg_latency_ms']) * 100:.2f}%\")\n",
    "print(f\"Size reduction: {(1 - pruned_metrics['model_size_mb'] / original_metrics['model_size_mb']) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1012dc99",
   "metadata": {},
   "source": [
    "> **Interpreting the results**: After pruning, you should observe:\n",
    "> \n",
    "> 1. Model Size Reduction: The pruned model should be smaller than the original\n",
    "> 2. Latency Improvement: Inference should be faster with fewer attention heads\n",
    "> 3. Accuracy Impact: There may be a small decrease in accuracy\n",
    "> \n",
    "> _The key question is:_ Does the trade-off make sense for your use case?\n",
    "> \n",
    "> - For mobile deployment, size reduction might be the priority\n",
    "> - For real-time applications, latency improvement might be most important\n",
    "> - For critical applications, maintaining accuracy might be non-negotiable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f2d78",
   "metadata": {},
   "source": [
    "# Step 9: Test with real-world examples\n",
    "\n",
    "Let's test our pruned model on some example sentences to verify it still works well in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test sentiment analysis\n",
    "def test_sentiment(model, tokenizer, sentences):\n",
    "    \"\"\"Test sentiment analysis on example sentences.\"\"\"\n",
    "    model.eval()\n",
    "    pipeline = TextClassificationPipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device if device.type != \"cpu\" else -1\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for sentence in sentences:\n",
    "        result = pipeline(sentence)\n",
    "        label = result[0]['label']\n",
    "        score = result[0]['score']\n",
    "        results.append((sentence, label, score))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with some examples\n",
    "test_sentences = [\n",
    "    \"I really loved this movie, it was fantastic!\",\n",
    "    \"The food was mediocre at best.\",\n",
    "    \"This book changed my life in many positive ways.\",\n",
    "    \"I'm feeling extremely disappointed with the service provided.\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting original model:\")\n",
    "original_results = test_sentiment(model, tokenizer, test_sentences)\n",
    "for sentence, label, score in original_results:\n",
    "    print(f\"'{sentence}' -> {label} ({score:.4f})\")\n",
    "\n",
    "print(\"\\nTesting pruned model:\")\n",
    "pruned_results = test_sentiment(pruned_model, tokenizer, test_sentences)\n",
    "for sentence, label, score in pruned_results:\n",
    "    print(f\"'{sentence}' -> {label} ({score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882e5df",
   "metadata": {},
   "source": [
    "> **From theory to practice**: Testing the pruned model on real examples is crucial to ensure that:\n",
    "> \n",
    "> 1. The model still produces correct classifications for obvious cases\n",
    "> 2. The confidence scores remain reasonable (not all 0.5 or all 1.0)\n",
    "> 3. The model handles different sentence styles and topics appropriately\n",
    "> \n",
    "> If the model's real-world performance is acceptable after pruning, it validates our approach. If not, we might need to:\n",
    "> \n",
    "> - Try a less aggressive pruning ratio\n",
    "> - Consider fine-tuning after pruning\n",
    "> - Experiment with alternative pruning criteri"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ff44fe6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you've learned how to:\n",
    "\n",
    "- Analyze attention head importance in transformer models using masking-based techniques\n",
    "- Implement structured pruning to remove redundant attention heads\n",
    "- Evaluate the performance impact of pruning\n",
    "- Make informed decisions about the accuracy-efficiency tradeoff\n",
    "\n",
    "These skills allow you to optimize transformer models for deployment in resource-constrained environments without retraining from scratch. Structured pruning is particularly valuable because it creates truly smaller models that run efficiently on standard hardware."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
