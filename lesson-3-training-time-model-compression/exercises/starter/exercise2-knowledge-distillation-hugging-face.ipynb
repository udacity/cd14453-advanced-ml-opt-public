{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a4016d8",
   "metadata": {},
   "source": [
    "# Exercise 2: Distill knowledge from a pre-trained language model to a smaller student model\n",
    "\n",
    "In machine learning, we often train large, powerful models that achieve high accuracy but are computationally expensive for deployment. Knowledge distillation is a powerful technique to transfer the 'knowledge' from a large \"teacher\" model to a smaller \"student\" model. The student learns to mimic the teacher's behavior, often retaining a significant portion of the teacher's performance while being much more efficient.\n",
    "\n",
    "> **Overview**: We will apply knowledge distillation to compress a pre-trained transformer model by training a smaller student model to reproduce the teacher's soft predictions on a classification task. This will result in a more efficient model suitable for resource-constrained environments.\n",
    ">\n",
    "> **Goal**: Implement knowledge distillation to create a smaller language model that maintains high accuracy while being more deployable for tasks like text classification.\n",
    ">\n",
    "> **Scenario**: Your SaaS company's support system uses a BERT model to identify duplicate customer tickets. While the system works perfectly, it's an overkill - the task is simple after all: determine if tickets like \"Can't login to my account\" and \"Unable to access my account\" are duplicates. And, yet, you are burning thousands in cloud costs for what should be a lightweight operation. So, you decide to simplify the model's architecture via knowledge distillation.\n",
    ">\n",
    "> **Tools**: pytorch, transformers, numpy, matplotlib\n",
    "> <br> _Prior experience with PyTorch and Transformers recommended!_\n",
    ">\n",
    "> **Estimated Time**: ~=20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca720b8",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3960297c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out the cell block again and restart the notebook\n",
    "# ! pip install transformers torch datasets matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Set the device for pytorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffc893",
   "metadata": {},
   "source": [
    "## Step 2: Load the teacher model and dataset\n",
    "\n",
    "We will use a fine-tuned BERT model as the teacher and a DistilBERT model as the student. DistilBERT is already significantly smaller than BERT, making it a good candidate for the student model. As they both share similar architectures, knowledge transfer is simplified.\n",
    "\n",
    "We'll load the [MRPC (Microsoft Research Paraphrase Corpus)](https://www.microsoft.com/en-us/download/details.aspx?id=52398) dataset, a common benchmark for paraphrase detection, which is a sequence classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the teacher model (larger BERT-based model)\n",
    "teacher_model_name = \"bert-base-uncased\"\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_model_name, \n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "# Load the student model (smaller DistilBERT)\n",
    "student_model_name = \"distilbert-base-uncased\"\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "student_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    student_model_name,\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Teacher model parameters: {sum(p.numel() for p in teacher_model.parameters()):,}\")\n",
    "print(f\"Student model parameters: {sum(p.numel() for p in student_model.parameters()):,}\")\n",
    "print(f\"Compression ratio: {sum(p.numel() for p in teacher_model.parameters()) / sum(p.numel() for p in student_model.parameters()):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a73db7e",
   "metadata": {},
   "source": [
    "> **Understanding initialization warnings**: These warnings are expected! Both BERT and DistilBERT are loaded with their pre-trained weights, but the classification heads are randomly initialized since we're adapting them for our specific task. This is why we need to fine-tune the models before using them for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ef031",
   "metadata": {},
   "source": [
    "> **Pros of pre-trained model access**: With just two lines of code, we're loading:\n",
    "> - **BERT-base**: 110M parameters trained on Wikipedia + BookCorpus\n",
    "> - **DistilBERT**: 66M parameters distilled from BERT by Hugging Face\n",
    "> \n",
    "> These models understand language structure, context, and semantics from their pre-training. We're just adapting them for our specific sentiment task. Without the transformers library, we'd need to:\n",
    "> 1. Implement the transformer architecture from scratch\n",
    "> 2. Download and manage 400MB+ model files manually\n",
    "> 3. Write custom tokenization code for each model\n",
    "> \n",
    "> The library handles all this complexity with simple, consistent APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d383d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"SetFit/mrpc\", split=\"train[:1000]\")\n",
    "print(f\"Dataset loaded!\\nNumber of examples: {len(dataset)}.\")\n",
    "print(f\"Features: {dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d544da",
   "metadata": {},
   "source": [
    "> **Understanding the MRPC dataset**:\n",
    "> The Microsoft Research Paraphrase Corpus contains pairs of sentences labeled as semantically equivalent (1) or not (0).\n",
    "> - **Features**: \n",
    ">   - `text1`, `text2`: The sentence pairs to compare\n",
    ">   - `label`: 0 (different meaning) or 1 (same meaning)\n",
    ">   - `label_text`: Human-readable version (\"not_equivalent\", \"equivalent\")\n",
    ">   - `idx`: Unique identifier for each example\n",
    "> \n",
    "> Also, about the _\"Repo card metadata block was not found\"_ warning: This warning simply means the dataset doesn't have a README card. It's harmless and doesn't affect the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a469cc0",
   "metadata": {},
   "source": [
    "# Step 3: Prepare the data for training\n",
    "\n",
    "Let's tokenize our dataset and create data loaders for training. \n",
    "\n",
    "Like all in-training compression techniques, knowledge distillation requires access to training data to teach the student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(dataset, tokenizer, max_length=128):\n",
    "    \"\"\"Tokenize text data for model input.\"\"\"\n",
    "    # MRPC dataset uses text1 and text2, not sentence1 and sentence2\n",
    "    texts = [ex[\"text1\"] + \" [SEP] \" + ex[\"text2\"] for ex in dataset]\n",
    "    labels = [ex[\"label\"] for ex in dataset]\n",
    "    \n",
    "    # Tokenize all texts\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return encoded, torch.tensor(labels)\n",
    "\n",
    "# Tokenize data for both models\n",
    "teacher_data, labels = tokenize_data(dataset, teacher_tokenizer)\n",
    "student_data, _ = tokenize_data(dataset, student_tokenizer)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "teacher_dataset = TensorDataset(\n",
    "    teacher_data[\"input_ids\"],\n",
    "    teacher_data[\"attention_mask\"],\n",
    "    labels\n",
    ")\n",
    "student_dataset = TensorDataset(\n",
    "    student_data[\"input_ids\"],\n",
    "    student_data[\"attention_mask\"],\n",
    "    labels\n",
    ")\n",
    "\n",
    "teacher_loader = DataLoader(teacher_dataset, batch_size=batch_size, shuffle=True)\n",
    "student_loader = DataLoader(student_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Data prepared: {len(teacher_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537f9de",
   "metadata": {},
   "source": [
    "> **Data flow in distillation**:\n",
    "> ```\n",
    "> Original Text â†’ BERT Tokenizer â†’ Teacher Model â†’ Soft Predictions â†˜\n",
    ">                                                                    â†’ Distillation Loss\n",
    "> Original Text â†’ DistilBERT Tokenizer â†’ Student Model â†’ Predictions â†—\n",
    "> ```\n",
    "> Both models see the same text but process it through their own tokenization pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45635469",
   "metadata": {},
   "source": [
    "# Step 4: Implement the knowledge distillation loss\n",
    "\n",
    "The core of knowledge distillation is the loss function. We need to implement a loss that combines:\n",
    "1. The standard cross-entropy loss against true labels (hard targets)\n",
    "2. The KL divergence between teacher and student predictions (soft targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc457b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, temperature=3.0, alpha=0.7):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        # TODO: Calculate the soft targets from logits for the teacher  using temperature\n",
    "        # Hint: Which PyTorch activation function converts logits to probabilities? How do you incorporate temperature?\n",
    "        # Find options at https://docs.pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions\n",
    "        # Consider using different ones for teacher and student too!\n",
    "        teacher_soft = # Add your code here\n",
    "        student_soft = # Add your code here\n",
    "        \n",
    "        # TODO: Calculate distillation loss\n",
    "        # Hint: Which PyTorch loss function measures divergence between two probability distributions?\n",
    "        #Â Find options at https://docs.pytorch.org/docs/stable/nn.functional.html#loss-functions\n",
    "        # Also, you should scale by temperature^2 as per the original paper\n",
    "        distillation_loss = # Add your code here\n",
    "        \n",
    "        # TODO: Calculate the loss with hard labels for the student\n",
    "        # Hint: What's the typical loss for classification? Note that we have set it up in our __init__ to support each forward pass\n",
    "        #Â Find options at https://docs.pytorch.org/docs/stable/nn.functional.html#loss-functions\n",
    "        hard_loss = # Add your code here\n",
    "        \n",
    "        # TODO: Combine both losses with alpha weighting\n",
    "        #  How do you balance two losses? One gets Î± weight, the other gets (1-Î±)\n",
    "        combined_loss = # Add your code here\n",
    "        \n",
    "        return combined_loss, distillation_loss, hard_loss\n",
    "\n",
    "# Initialize the distillation loss\n",
    "criterion = DistillationLoss(temperature=3.0, alpha=0.7)\n",
    "print(\"Distillation loss initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f4467",
   "metadata": {},
   "source": [
    "> **Understanding temperature in distillation**: Temperature is a crucial hyperparameter in knowledge distillation. Higher temperatures (T > 1) \"soften\" the probability distributions, making small differences between classes more pronounced. This helps the student learn the nuanced relationships between classes that the teacher has discovered. A temperature of 3.0 is a good starting point, but you may need to tune this based on your specific task.\n",
    "> \n",
    "> Let's take this sentence pair as an exmaple: \"Great movie!\" vs \"Excellent film!\"\n",
    "> \n",
    ">   - _Without temperature (T=1)_:  \n",
    ">   Teacher output: [0.02, 0.98] (very confident: same sentiment)    \n",
    ">   Student learns: \"These are definitely the same\"\n",
    "> <br><br>\n",
    ">   - _With temperature (T=3)_:  \n",
    ">   Teacher output: [0.15, 0.85] (confident but nuanced)  \n",
    ">   Student learns: \"These are likely the same, but there's slight uncertainty\"\n",
    "> \n",
    "> The temperature-softened distribution teaches the student about the teacher's uncertainty and edge cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77504822",
   "metadata": {},
   "source": [
    "## Step 5: Implement the model training loop\n",
    "\n",
    "Now we'll implement the training loop that teaches the student model to mimic the teacher's behavior. This is where the actual knowledge transfer happens.\n",
    "\n",
    "**Important**: The training configuration is hard-coded, but feel free to experiment with any training parameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the number of epochs\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04680f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's fine-tune the teacher model on our task (for fair comparison)\n",
    "print(\"Fine-tuning the teacher model first...\")\n",
    "\n",
    "def train_teacher(model, train_loader, device, num_epochs):\n",
    "    \"\"\"Fine-tune the teacher model on the task.\"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        progress_bar = tqdm(train_loader, desc=f'Teacher Epoch {epoch+1}/{num_epochs}')\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for input_ids, attention_mask, labels in progress_bar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{accuracy:.2f}%'\n",
    "            })\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Teacher Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Fine-tune the teacher first\n",
    "teacher_model = train_teacher(teacher_model, teacher_loader, device, num_epochs=num_epochs)\n",
    "print(\"\\nTeacher model fine-tuned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4e4cc",
   "metadata": {},
   "source": [
    "> **Why fine-tune the teacher first?**\n",
    "> - Pre-trained BERT knows general language but not our specific task\n",
    "> - Fine-tuning creates task-specific knowledge to transfer\n",
    "> - Without this, we'd be distilling general language understanding only\n",
    "> \n",
    "> And, we can see how the teacher becomes an \"expert\" at movie review sentiment across training epochs:\n",
    "> - Epoch 1 (66.5%): Learning basic sentiment patterns\n",
    "> - Epoch 2 (77.5%): Refining decision boundaries\n",
    "> - Epoch 3 (88.2%): Achieving strong task performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c2a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_distillation(student_model, teacher_model, train_loader, criterion, \n",
    "                           optimizer, device, num_epochs):\n",
    "    \"\"\"Train student model using knowledge distillation from teacher model.\"\"\"\n",
    "    \n",
    "    # TODO: Make sure the teacher does not train\n",
    "    # Hint: We want consistent predictions from the teacher - which mode prevents dropout and batch norm updates?\n",
    "    # Add your one-line code here\n",
    "    \n",
    "    training_history = {\n",
    "        'total_loss': [],\n",
    "        'distillation_loss': [],\n",
    "        'hard_loss': [],\n",
    "        'accuracy': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        student_model.train()\n",
    "        epoch_losses = {'total': 0, 'distillation': 0, 'hard': 0}\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Create progress bar for batches\n",
    "        progress_bar = tqdm(enumerate(train_loader), \n",
    "                           total=len(train_loader),\n",
    "                           desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, (input_ids, attention_mask, labels) in progress_bar:\n",
    "            # Move data to device\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # TODO: Get teacher predictions (no gradients needed)\n",
    "            # Hint: Use torch.no_grad() context and forward pass through teacher\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = # Add your code here\n",
    "                teacher_logits = # Add your code here\n",
    "            \n",
    "            # TODO: Get student predictions\n",
    "            # Hint: Forward pass through student model\n",
    "            student_outputs = # Add your code here\n",
    "            \n",
    "            # TODO: Calculate distillation loss\n",
    "            # Hint: Use the criterion we defined earlier\n",
    "            loss, distill_loss, hard_loss = # Add your code here\n",
    "            \n",
    "            # Standard backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            epoch_losses['total'] += loss.item()\n",
    "            epoch_losses['distillation'] += distill_loss.item()\n",
    "            epoch_losses['hard'] += hard_loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(student_logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_acc = 100 * correct / total\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{current_acc:.2f}%'\n",
    "            })\n",
    "        \n",
    "        # Record epoch metrics\n",
    "        avg_losses = {k: v/len(train_loader) for k, v in epoch_losses.items()}\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        training_history['total_loss'].append(avg_losses['total'])\n",
    "        training_history['distillation_loss'].append(avg_losses['distillation'])\n",
    "        training_history['hard_loss'].append(avg_losses['hard'])\n",
    "        training_history['accuracy'].append(accuracy)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"Average Total Loss: {avg_losses['total']:.4f}\")\n",
    "        print(f\"Average Distillation Loss: {avg_losses['distillation']:.4f}\")\n",
    "        print(f\"Average Hard Loss: {avg_losses['hard']:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\\n\")\n",
    "    \n",
    "    return training_history\n",
    "\n",
    "# Set up optimizer for student model\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=2e-5)\n",
    "\n",
    "# Train the student model\n",
    "print(\"Starting knowledge distillation training...\")\n",
    "history = train_with_distillation(\n",
    "    student_model, teacher_model, student_loader, \n",
    "    criterion, optimizer, device, num_epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a449a8",
   "metadata": {},
   "source": [
    "> **Recap: Distillation training strategy**:\n",
    "> 1. *Teacher in eval mode*: No dropout, stable predictions\n",
    "> 2. *Two forward passes*: Teacher (no grad) + Student (with grad)\n",
    "> 3. *Complex loss*: Balancing soft targets and hard labels\n",
    "> 4. *Progress tracking*: Monitoring three losses instead of one\n",
    "> \n",
    "> So, we can summarize the knowledge transfer process as follows:\n",
    "> ```\n",
    "> Teacher (frozen) â†’ Consistent soft predictions\n",
    ">                    â†“\n",
    "> Student (learning) â†’ Tries to match teacher's confidence patterns\n",
    ">                    â†“\n",
    "> Loss backprop â†’ Student parameters update\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed0476",
   "metadata": {},
   "source": [
    "## Step 6: Visualize the training history\n",
    "\n",
    "Let's visualize how the different loss components evolve during training to understand how the student learns from the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot losses\n",
    "# TODO: Define losses\n",
    "# Hint: This information is stored in the model training `history`!\n",
    "total_loss, distillation_loss, hard_loss =     # Add your code here\n",
    "epochs = range(1, len(total_loss) + 1)\n",
    "ax1.plot(epochs, total_loss, 'b-', label='Total Loss')\n",
    "ax1.plot(epochs, distillation_loss, 'r--', label='Distillation Loss')\n",
    "ax1.plot(epochs, hard_loss, 'g:', label='Hard Target Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Losses During Distillation')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(epochs, history['accuracy'], 'b-', marker='o')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Student Model Accuracy')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final student accuracy: {history['accuracy'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b45cd",
   "metadata": {},
   "source": [
    "> **Interpreting the loss curves**: The visualization shows how the student balances learning from both the teacher (distillation loss) and the true labels (hard loss). Initially, the distillation loss dominates as the student learns the teacher's behavior patterns. As training progresses, both losses decrease, indicating the student is successfully learning to mimic the teacher while maintaining accuracy on the actual task.\n",
    "> \n",
    "> Both the teacher and student model in this example have not converged; so, in reality, you'd train for more and see even further performance improvements!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f2d78",
   "metadata": {},
   "source": [
    "# Step 7: Compare student vs teacher performance\n",
    "\n",
    "Now let's evaluate both models to see how much performance the student retained while being significantly smaller. Then, let's visualize it to better understand tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a110f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset_split, device):\n",
    "    \"\"\"Evaluate model performance on a dataset with consistent preprocessing.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_time = 0\n",
    "    \n",
    "    # Create a proper test set\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    \n",
    "    for example in dataset_split:\n",
    "        test_texts.append(example[\"text1\"] + \" [SEP] \" + example[\"text2\"])\n",
    "        test_labels.append(example[\"label\"])\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_size = 16\n",
    "    num_batches = 0\n",
    "    \n",
    "    for i in range(0, len(test_texts), batch_size):\n",
    "        batch_texts = test_texts[i:i+batch_size]\n",
    "        batch_labels = test_labels[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize for this specific model\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        batch_labels_tensor = torch.tensor(batch_labels).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            outputs = model(**inputs)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            total_time += (end_time - start_time)\n",
    "            num_batches += 1\n",
    "            \n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += len(batch_labels)\n",
    "            correct += (predicted == batch_labels_tensor).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_time = (total_time / num_batches) * 1000  # ms per batch\n",
    "    \n",
    "    return accuracy, avg_time\n",
    "\n",
    "# Create the test split\n",
    "test_split = dataset.select(range(800, 1000))\n",
    "\n",
    "print(\"Evaluating both models on the same test set...\")\n",
    "teacher_acc, teacher_time = evaluate_model(teacher_model, teacher_tokenizer, \n",
    "                                               test_split, device)\n",
    "student_acc, student_time = evaluate_model(student_model, student_tokenizer, \n",
    "                                               test_split, device)\n",
    "\n",
    "# Calculate compression metrics\n",
    "# TODO: Calculate the parameter reduction percentage\n",
    "# Hint: Parameter reduction = (1 - student_params/teacher_params) * 100\n",
    "# See: https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html and https://docs.pytorch.org/docs/stable/generated/torch.numel.html \n",
    "param_reduction =  # Add your code here\n",
    "speed_improvement = (teacher_time - student_time) / teacher_time * 100\n",
    "accuracy_change = student_acc - teacher_acc\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"Teacher accuracy: {teacher_acc:.2f}%\")\n",
    "print(f\"Student accuracy: {student_acc:.2f}%\")\n",
    "print(f\"Teacher inference time: {teacher_time:.2f} ms/batch\")\n",
    "print(f\"Student inference time: {student_time:.2f} ms/batch\")\n",
    "print(f\"\\nCompression Results:\")\n",
    "print(f\"Parameter reduction: {param_reduction:.1f}%\")\n",
    "print(f\"Speed improvement: {speed_improvement:.1f}%\")\n",
    "print(f\"Accuracy change: +{accuracy_change:.2f}%\") if accuracy_change >= 0 else print(f\"Accuracy change: {accuracy_change:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34f96c",
   "metadata": {},
   "source": [
    "> **Can the student overcome the teacher?**: It is unusual, but it can happen! Reasons include:\n",
    ">   - Small test set (200 examples) -> High variance\n",
    ">   - Regularization effect of distillation\n",
    ">   - Lucky initialization for student model\n",
    ">   - Teacher model not converged yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Model size comparison\n",
    "models = ['Teacher\\n(BERT)', 'Student\\n(DistilBERT)']\n",
    "sizes = [\n",
    "    sum(p.numel() for p in teacher_model.parameters()) / 1e6,\n",
    "    sum(p.numel() for p in student_model.parameters()) / 1e6\n",
    "]\n",
    "ax1.bar(models, sizes, color=['#3498db', '#2ecc71'])\n",
    "ax1.set_ylabel('Parameters (millions)')\n",
    "ax1.set_title('Model Size Comparison')\n",
    "ax1.set_ylim(0, max(sizes) * 1.2)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, size in enumerate(sizes):\n",
    "    if i == 1:\n",
    "        reduction = (1 - sizes[1]/sizes[0]) * 100\n",
    "        ax1.text(i, size + 2, f'{size:.1f}M\\n(-{reduction:.1f}%)', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    else:\n",
    "        ax1.text(i, size + 2, f'{size:.1f}M', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Accuracy comparison\n",
    "accuracies = [teacher_acc, student_acc]\n",
    "ax2.bar(models, accuracies, color=['#3498db', '#2ecc71'])\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Model Accuracy')\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "# Add accuracy labels\n",
    "for i, acc in enumerate(accuracies):\n",
    "    ax2.text(i, acc + 1, f'{acc:.1f}%', \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Inference time comparison\n",
    "times = [teacher_time, student_time]\n",
    "ax3.bar(models, times, color=['#3498db', '#2ecc71'])\n",
    "ax3.set_ylabel('Time (ms/batch)')\n",
    "ax3.set_title('Inference Speed')\n",
    "ax3.set_ylim(0, max(times) * 1.2)\n",
    "\n",
    "# Add time labels\n",
    "for i, time in enumerate(times):\n",
    "    if i == 1:\n",
    "        speedup = (times[0] - times[1]) / times[0] * 100\n",
    "        ax3.text(i, time + 0.5, f'{time:.1f}ms\\n({speedup:.1f}% faster)', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    else:\n",
    "        ax3.text(i, time + 0.5, f'{time:.1f}ms', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Temperature effect visualization\n",
    "temperatures = [1.0, 3.0, 5.0, 10.0]\n",
    "softmax_diffs = []\n",
    "\n",
    "# Simulate the effect of temperature on probability distributions\n",
    "sample_logits = torch.tensor([2.0, 1.0, 0.5])\n",
    "for temp in temperatures:\n",
    "    soft_probs = F.softmax(sample_logits / temp, dim=0)\n",
    "    # Calculate entropy as a measure of distribution smoothness\n",
    "    entropy = -(soft_probs * torch.log(soft_probs + 1e-8)).sum().item()\n",
    "    softmax_diffs.append(entropy)\n",
    "\n",
    "ax4.plot(temperatures, softmax_diffs, 'ro-', linewidth=2, markersize=8)\n",
    "ax4.set_xlabel('Temperature')\n",
    "ax4.set_ylabel('Distribution Entropy')\n",
    "ax4.set_title('Effect of Temperature on Softmax')\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b882e5df",
   "metadata": {},
   "source": [
    "> **Understanding temperature in distillation**: Temperature (T) is a scaling factor that \"softens\" probability distributions. The graph above simulates temperature effects in distillation by applying different T values (1.0-10.0) to sample logits [2.0, 1.0, 0.5] and measuring the resulting entropy. Higher temperatures create flatter distributions, revealing more of the teacher's \"dark knowledge\" for the student to learn. Higher entropy means more nuanced information available for the student to learn from.\n",
    "> \n",
    "> **Brainstorming stop**: How much money could this compression save? Well, it depends on the requests pattern!\n",
    "> If you're curious, try to pick a cloud provider and check what the final cost saving for 50k queries/day could look like!\n",
    "> \n",
    "> ðŸ’¡ *Hint*: Cloud providers often charge by compute-hour. So, you need to carefully choose your deployment machine type."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ff44fe6",
   "metadata": {},
   "source": [
    "## Step 8: Test with some example queries\n",
    "\n",
    "Let's test both models on some example support queries to see how they perform in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbffa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on example queries\n",
    "test_queries = [\n",
    "    (\"I can't log into my account\", \"Unable to access my account\"),\n",
    "    (\"Password reset isn't working\", \"Can't reset my password\"),\n",
    "    (\"App crashes on startup\", \"Application fails to launch\"),\n",
    "    (\"Subscription won't cancel\", \"Unable to unsubscribe\"),\n",
    "    (\"Feature request: dark mode\", \"Please add dark theme option\"),\n",
    "    (\"Billing error this month\", \"Incorrect charge on my bill\"),\n",
    "    (\"Can't upload large files\", \"File upload fails for big documents\"),\n",
    "    (\"Missing export button\", \"Where is the download option?\"),\n",
    "    (\"Account locked after update\", \"Software update locked me out\"),\n",
    "    (\"Refund still not received\", \"Where's my money back?\")\n",
    "]\n",
    "\n",
    "print(\"Testing on customer support ticket pairs:\\n\")\n",
    "\n",
    "for i, (query1, query2) in enumerate(test_queries):\n",
    "    print(f\"Query pair {i+1}:\")\n",
    "    print(f\"Q1: {query1}\")\n",
    "    print(f\"Q2: {query2}\")\n",
    "    \n",
    "    # Tokenize for both models\n",
    "    teacher_inputs = teacher_tokenizer(\n",
    "        query1 + \" [SEP] \" + query2,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    student_inputs = student_tokenizer(\n",
    "        query1 + \" [SEP] \" + query2,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        teacher_output = teacher_model(**teacher_inputs)\n",
    "        student_output = student_model(**student_inputs)\n",
    "        \n",
    "        teacher_probs = F.softmax(teacher_output.logits, dim=1)\n",
    "        student_probs = F.softmax(student_output.logits, dim=1)\n",
    "        \n",
    "        teacher_pred = torch.argmax(teacher_probs).item()\n",
    "        student_pred = torch.argmax(student_probs).item()\n",
    "    \n",
    "    labels = [\"Different\", \"Similar\"]\n",
    "    print(f\"Teacher: {labels[teacher_pred]} (confidence: {teacher_probs[0][teacher_pred]:.3f})\")\n",
    "    print(f\"Student: {labels[student_pred]} (confidence: {student_probs[0][student_pred]:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a914b1",
   "metadata": {},
   "source": [
    "> **Nice, distillation worked!** The student learned the teacher's style but kept some independence. \n",
    "> Their answers almost always coincide, with exception of some nuanced use cases.\n",
    "> \n",
    "> What's interesting is looking at confidence levels too. The student seems to have learned to be:\n",
    "> - More decisive on technical terminology (password, app, crashes)\n",
    "> - More hesitant on conceptual differences (upload vs files, export vs download)\n",
    "> \n",
    "> Through distillation, the student developed its own \"personality\" while still matching the teacher's overall accuracy. It's like teaching someone chess and they develop their own playing style!\n",
    "> \n",
    "> _How would you handle this for real use?_ You'd definetely run more extensive testing! And, then, a good strategy could be to auto-merge anything over 70% confidence and let humans handle the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b41a9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you've learned how to:\n",
    "* Implement knowledge distillation loss functions combining soft and hard targets\n",
    "* Train student models to mimic teacher behavior through temperature-scaled predictions\n",
    "* Balance distillation and ground truth losses using alpha weighting\n",
    "* Create smaller models that preserve teacher's nuanced understanding\n",
    "* Evaluate compression trade-offs between model size, speed, and accuracy\n",
    "\n",
    "These skills allow you to create inherently efficient models through training-time compression, rather than post-hoc optimization. Knowledge distillation is particularly valuable when changing architectures (BERT to DistilBERT) or when preserving subtle task understanding is critical for production quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
