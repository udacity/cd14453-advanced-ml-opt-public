{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "intro_quant_ex_revised",
   "metadata": {},
   "source": [
    "# Exercise: Implement quantization-aware training in TensorFlow\n",
    "\n",
    "In-training quantization (or Quantization Aware Training - QAT) is like teaching someone a new skill while using the tools they'll actually have—similar to training an athlete at high altitude when they'll compete there, not at sea level. The model learns to perform well with quantized weights and activations right from the start.\n",
    "\n",
    "> **Overview**: We'll apply quantization aware training to an image classification model, allowing it to learn how to maintain accuracy despite reduced numerical precision. This technique produces models that excel when deployed on hardware optimized for integer operations.\n",
    "> \n",
    "> **Goal**: Implement QAT to create highly optimized integer models that can achieve peak performance on specialized hardware and potentially recover subtle accuracy losses compared to post-training quantization methods.\n",
    "> \n",
    "> **Scenario**: You work for a wildlife conservation organization deploying smart camera traps in remote areas (same as for Lesson 2). These battery-powered cameras need to identify different animal species locally to prioritize important footage for transmission back to base via limited satellite connectivity. Your team has a pre-trained MobileNetV2 model that accurately identifies local species for which they have applied post-training quantization (PQT). While the model works well in testing, field deployment faces two critical challenges:\n",
    ">    1. _On-device performance gap_: While PTQ models were fast, on the *actual camera trap hardware*, which features a specialized Neural Processing Unit (NPU) optimized for 8-bit integer (INT8) operations, the team believes even greater latency reduction and power efficiency are achievable if the model is *perfectly* conditioned for INT8.\n",
    ">    2. _Accuracy improvements for critical cases_: For a newly discovered and extremely rare bird species, which shares very subtle visual features with a common one, the PTQ INT8 model showed a tiny, but critical, dip in distinguishing accuracy compared to the float TFLite model.\n",
    "> \n",
    "> Your team has selected Quantization Aware Training (QAT) as the next step. By training with simulated quantization, the model will learn to be robust to INT8 constraints while maintaining critical accuracy for rare species identification.\n",
    "> \n",
    "> **Tools**: TensorFlow, TensorFlow Model Optimization Toolkit, LiteRT, MobileNetV2, Matplotlib.\n",
    "> \n",
    "> **Estimated Time**: 15 minutes\n",
    "\n",
    "**⚠️ NOTE**: We skip model accuracy benchmarking for this demo to focus on the technique only, please feel free to expand!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_quant_ex_title_revised",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pip_install_quant_ex_revised",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install necessary libraries, then comment out the cell block again and restart the notebook\n",
    "# ! pip install 'tensorflow[and-cuda]' datasets tensorflow_datasets tensorflow-model-optimization tf_keras tflite_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for TensorFlow for the session\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Avoid logging info and warnings\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'  # Ensure tf_keras is used for compatibility with tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_quant_ex_revised",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Set random seed for deterministic results\n",
    "os.environ['PYTHONHASHSEED']=str(2)\n",
    "tf.random.set_seed(2)\n",
    "np.random.seed(2)\n",
    "random.seed(2)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"assets/exercise1\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb1f7a",
   "metadata": {},
   "source": [
    "> **Environment configuration**: We set `TF_USE_LEGACY_KERAS=1` to ensure compatibility with the TensorFlow Model Optimization toolkit, which may not yet fully support Keras 3.0. Setting random seeds ensures reproducible results across runs.\n",
    "> \n",
    "> **About those warnings**: The CUDA-related warnings (cuFFT, cuDNN, cuBLAS) appear because TensorFlow is attempting to register GPU libraries multiple times for the multiple TensorFlow components that are loaded  - this is harmless and won't affect your model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_model_quant_ex_title_revised",
   "metadata": {},
   "source": [
    "## Step 2: Load the pre-trained model\n",
    "We'll load a MobileNetV2 model pre-trained on ImageNet. \n",
    "\n",
    "If the scenario was real, this would be your custom-trained wildlife classification model that has already undergone initial evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model_quant_ex_code_revised",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained MobileNetV2 model\n",
    "input_shape = (224, 224, 3)\n",
    "base_model = MobileNetV2(input_shape=input_shape, weights='imagenet', include_top=True)\n",
    "\n",
    "# Save the original model for comparison\n",
    "original_model_path = os.path.join(output_dir, \"mobilenetv2_original.keras\")\n",
    "base_model.save(original_model_path)\n",
    "\n",
    "# Get some model information\n",
    "print(f\"Original MobileNetV2 model saved to {original_model_path}\")\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ae7ae",
   "metadata": {},
   "source": [
    "> **A look at the model architecture**: The repeating \"block_X\" patterns reveal MobileNetV2's inverted residual structure. These blocks expand channels internally (up to 960) before projecting back to smaller dimensions (96-160), enabling rich feature extraction within memory constraints.\n",
    "> \n",
    "> _For further exploration_, you could execute `[layer.name for layer in base_model.layers if 'expand' in layer.name]` to identify expansion layers, or use `tf.keras.utils.plot_model(base_model, show_shapes=True)` to visualize the bottleneck architecture. This expansion-projection pattern explains how a 13MB model achieves high accuracy—critical for resource-limited wildlife camera deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qat_model_quant_ex_title_revised",
   "metadata": {},
   "source": [
    "## Step 3: Define the quantization-aware model\n",
    "Now we'll apply quantization-aware training. This step modifies the model to simulate quantization during the forward pass while maintaining full precision during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qat_model_quant_ex_code_revised",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Apply quantization aware training to the base_model.\n",
    "# Hint: Look for built-in functionalities in tensorflow-model-optimization. \n",
    "# Documentation is at https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/quantization/keras.\n",
    "q_aware_model = # Add your code here\n",
    "\n",
    "print(\"Quantization-aware model created!\")\n",
    "print(\"New layers added for fake quantization during training.\")\n",
    "\n",
    "# TODO: Compile the quantization-aware model with appropriate settings (loss, optimizer, metrics, ...)\n",
    "# Add your code here\n",
    "\n",
    "print(\"Quantization-aware model created and compiled.\")\n",
    "\n",
    "# Let's examine the structure - you'll see new QuantizeLayer components\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0811299",
   "metadata": {},
   "source": [
    "> **How QAT works**: Notice how every layer now has a \"quant_\" prefix? When you make your model quantization-aware, you wrap each layer with fake quantization operations that simulate INT8 behavior (i.e., reducing precision) during training (but only in the forward pass!) to allow the model to learn weights that are robust to quantization. \n",
    "> <br> _Important_: Look at the parameter counts - Conv1 went from 864 to 929 parameters, indicating the wrapper adds quantization metadata.  The `quantize_layer` at the input and the per-layer parameter increases (typically +3 for quantization parameters) show how comprehensively the model has been prepared for INT8 training.\n",
    "> \n",
    "> **Why open-source frameworks are so important**: Without automated tools, you'd need to manually insert quantization operations after every weight computation and activation function throughout the entire network - potentially 100+ insertion points in MobileNetV2. Each would require careful calibration to avoid breaking the model's numerical stability. Thank you, open-source!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_qat_model_quant_ex_title_revised",
   "metadata": {},
   "source": [
    "## Step 4: Train (Fine-tune) the quantization-aware model\n",
    "With QAT, we typically fine-tune a pre-trained model for a few epochs on the original training set of the baseline model. The model learns to adjust its weights to minimize accuracy loss from quantization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712fc7ca",
   "metadata": {},
   "source": [
    "### Load and prepare the data\n",
    "\n",
    "For this exercise, we'll use an animal classification dataset called [Animal_Image_Classification](https://github.com/AlvaroVasquezAI/Animal_Image_Classification) as a proxy for wildlife camera trap data. For this scenario, data would be in higher quantity and also of higher resolution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f2ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and shuffle the dataset\n",
    "print(\"Loading the data...\")\n",
    "dataset = load_dataset(\"AlvaroVasquezAI/Animal_Image_Classification_Dataset\")[\"train\"]\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Split in train, test, and validation\n",
    "print(\"Splitting the processed dataset into train, test, and validation...\")\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_data = dataset.select(range(0, train_size))\n",
    "val_data = dataset.select(range(train_size, train_size + val_size))\n",
    "test_data = dataset.select(range(train_size + val_size, total_size))\n",
    "\n",
    "# Convert and preprocess datasets\n",
    "batch_size = 32\n",
    "def preprocess(entry, label):\n",
    "    image = tf.image.resize(entry['image'], (224, 224))  # Matches MobileNetV2's expected input\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)  # Applies ImageNet's normalization stats to keep inputs in the same distribution the model originally learned\n",
    "    return image, label\n",
    "\n",
    "def convert_to_tf_dataset(hf_data, batch_size=batch_size):\n",
    "    ds = hf_data.to_tf_dataset(columns=[\"image\", \"label\"], label_cols=\"label\")\n",
    "    ds = ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Converting and preprocessing splits...\")\n",
    "train_dataset = convert_to_tf_dataset(train_data)\n",
    "val_dataset = convert_to_tf_dataset(val_data)\n",
    "test_dataset = convert_to_tf_dataset(test_data)\n",
    "\n",
    "# Check the sizes of the datasets\n",
    "print(f\"Train size: {train_size}, Val size: {val_size}, Test size: {test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3081a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample\n",
    "for (images, labels) in test_dataset.take(1):\n",
    "    # Extract first image and label from batch\n",
    "    image, label = images[0], labels[0]\n",
    "    \n",
    "    # Get image shape\n",
    "    print(\"Image shape:\", image.shape)\n",
    "\n",
    "    # Convert the image from tensor to numpy array for visualization\n",
    "    plt.imshow(image.numpy())\n",
    "    plt.title(f\"Label: {label.numpy()}\")\n",
    "    plt.show()\n",
    "    \n",
    "    break  # Stop after the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7742f7",
   "metadata": {},
   "source": [
    "> **Data preprocessing with TensorFlow**: The `.prefetch()` and parallel mapping ensure your GPU never waits for data - critical when training with quantization overhead. `tf.data` provides other cool performance optimization options, if you're interested to [learn more](https://www.tensorflow.org/guide/data_performance)!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb6b56b6",
   "metadata": {},
   "source": [
    "### Train the model with QAT\n",
    "Training will be very short here—it's just an exercise aimed to demonstrate the technique, so definetely not deployment-ready! Real QAT fine-tuning might take longer and require careful hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_qat_model_quant_ex_code_revised",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training QAT model with {train_size} training samples...\")\n",
    "# TODO: Train (fine-tune) the q_aware_model with quantization awareness.\n",
    "# Hint: Use the `fit` method with some desired parameters (don't worry if model performance is not great!)\n",
    "# Do keep a low epochs count (3-5) as each epoch takes 2-3 minutes to complete.\n",
    "history = # Add your code here\n",
    "\n",
    "print(\"QAT model fine-tuning complete.\")\n",
    "\n",
    "# Save model\n",
    "model_save_path = os.path.join(output_dir, \"mobilenetv2_qat.h5\")\n",
    "q_aware_model.save(model_save_path)\n",
    "\n",
    "# Plot training accuracy\n",
    "# TODO: Add the number of epochs you have trained for; expected an integer.\n",
    "num_epochs = # Add your value here\n",
    "if 'accuracy' in history.history and 'val_accuracy' in history.history:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('QAT Model Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(ticks=range(num_epochs), labels=[str(i + 1) for i in range(num_epochs)])  # Re-label to start from 1 rather than 0\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d32d975",
   "metadata": {},
   "source": [
    "> **What can you notice from the training**: We are running into a classic transfer learning problem: training accuracy quickly skyrockets while validation lag (i.e., model is overfitting). With more epochs, the model would learn to better adapt to quantization for our specific task. \n",
    "> <br> Also, the first epoch takes longer (173s vs 127s). Why? XLA compilation and quantization graph optimization\n",
    "> \n",
    "> _**Don't worry about fixing it! Just take some time to think about what would be best to do.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convert_qat_model_quant_ex_title_revised",
   "metadata": {},
   "source": [
    "## Step 5: Convert to a LiteRT model\n",
    "After QAT, the model is ready to be converted into a LiteRT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convert_qat_model_quant_ex_code_revised",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Converting model to Tensorflow\")\n",
    "\n",
    "# Load the saved QAT model\n",
    "model_save_path = os.path.join(output_dir, \"mobilenetv2_qat.h5\")\n",
    "with tfmot.quantization.keras.quantize_scope():\n",
    "    q_aware_model = tf.keras.models.load_model(model_save_path)\n",
    "\n",
    "# TODO: Create a TFLiteConverter from the saved q_aware_model.\n",
    "# Hint: Review LiteRT documentation at https://ai.google.dev/edge/api/tflite/python/tf/lite\n",
    "converter = # Add your code here\n",
    "\n",
    "# TODO: Set up the converter arguments as desired (optimizations, inference_input_type, ...)\n",
    "# Hint: You have to set up `converter.optimizations` and `converted.representative_dataset` at minimum.\n",
    "# For the latter, we provide a skeleton for the function for you to complete. \n",
    "converter.optimizations = # Add your code here\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    # Add your code here as one-line for each missing step\n",
    "    # 1. Iterate through 1 batch of images from the dataset (in practice, you'd use more!)\n",
    "    # 2. Apply the same preprocessing used during training\n",
    "    # 3. [If required] Add batch dimension (model expects shape [1, 224, 224, 3])\n",
    "    \n",
    "    # 4. Ensure the image shape is correct for the model\n",
    "    assert image.shape == (batch_size, 224, 224, 3), f\"Expected shape ({batch_size}, 224, 224, 3), got {image.shape}\"\n",
    "        \n",
    "    # 5. Yield input (as required by TFLite!)\n",
    "\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "\n",
    "# Convert the model\n",
    "tflite_qat_model = converter.convert()\n",
    "\n",
    "# Save the QAT TFLite model\n",
    "tflite_qat_model_path = os.path.join(output_dir, \"mobilenetv2_qat.tflite\")\n",
    "with open(tflite_qat_model_path, 'wb') as f:\n",
    "    f.write(tflite_qat_model)\n",
    "\n",
    "print(f\"QAT TFLite model saved to {tflite_qat_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e5619d",
   "metadata": {},
   "source": [
    "> **Why is the representative dataset needed?**: If you tried without it, the converter would crash. That's because the final TFLite conversion still must determine exact INT8 ranges for any remaining float operations, activation functions, and layer outputs that have not been fully quantized during QAT training (e.g., softmax or custom layers). The converter needs actual data flowing through the model to calculate these ranges - without it, it encounters null/invalid tensors and fails.\n",
    ">\n",
    "> **Understanding the warnings**: The TensorFlow warnings are ok to ignore. `Ignored output_format`, `Ignored drop_control_dependency` and `MLIR V1 optimization pass` indicate setting that are ignored in the conversion process, which is ok if we're not actively configuring them.  \n",
    "> After conversion, TensorFlow logs the input and output format too. If you don't specify `inference_input_type` and `inference_output_type` in your conversion, these would be FLOAT32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_tflite_quant_ex_title_revised",
   "metadata": {},
   "source": [
    "## Step 6: Run inference with the QAT model\n",
    "Let's test our INT8 model to ensure it loads correctly and examine its properties.\n",
    "\n",
    "In practice, we'd need to run a full evaluation of the `.tflite` model across all metrics, ideally on the target device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_tflite_quant_ex_code_revised",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lite interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_qat_model_path)\n",
    "\n",
    "# TODO: Allocate tensors for the interpreter\n",
    "# Hint: Check the interpreter's documentation at https://ai.google.dev/edge/api/tflite/python/tf/lite/Interpreter\n",
    "# Add your code here\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "print(\"\\nQAT TFLite Model Input Details:\", input_details)\n",
    "print(\"QAT TFLite Model Output Details:\", output_details)\n",
    "\n",
    "# Test inference with a sample input\n",
    "# If input_details['dtype'] is int8, we need to derive the scale and zero-point from the quantization parameters\n",
    "# to make the input into the int8 range [-128, 127]\n",
    "if input_details['dtype'] == np.int8:\n",
    "    # For INT8 input, we need to quantize our test data\n",
    "    scale = input_details['quantization'][0]\n",
    "    zero_point = input_details['quantization'][1]\n",
    "    print(f\"\\nQuantization parameters - Scale: {scale}, Zero point: {zero_point}\")\n",
    "    \n",
    "    # Create a test input (normally you'd quantize real data)\n",
    "    test_input = np.random.randint(-128, 127, size=input_details['shape'], dtype=np.int8)\n",
    "else: # Fallback for float models, though QAT aims for int\n",
    "    test_input = np.random.random(input_details['shape']).astype(input_details['dtype'])\n",
    "\n",
    "# TODO: Set the input tensor for the interpreter instance (it's an attribute!)\n",
    "# Hint: Check the interpreter's documentation at https://ai.google.dev/edge/api/tflite/python/tf/lite/Interpreter\n",
    "# Add your code here  \n",
    "\n",
    "# Invoke the interpreter\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output\n",
    "output_data = interpreter.get_tensor(output_details['index'])\n",
    "\n",
    "print(f\"\\nSuccessfully ran inference with QAT TFLite model.\")\n",
    "print(f\"Output shape: {output_data.shape}, Output dtype: {output_data.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ac0389",
   "metadata": {},
   "source": [
    "> **Reading int8 results**: The model inputs and outputs are typically quantized/dequantized automatically by the TFLite runtime. If you keep the inputs and outputs in `float32` with empty quantization parameters (`scales: array([])`, `zero_points: array([])`), this can keep the API float-friendly for compatibility while LiteRT still handles INT8 conversion internally.\n",
    "> <br> For INT8 models, inputs are quantized using the scale/zero-point parameters shown above, and outputs are dequantized back to float32 by default. However, some edge devices keep everything in INT8 for maximum efficiency - in those cases, you'd need manual dequantization: `float_value = (int_value - zero_point) * scale`. \n",
    "> <br> So, if your model produces unexpected outputs on your target device, check whether your hardware/runtime is doing automatic dequantization - this varies by platform and can cause confusion when comparing Float32 vs INT8 model results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fded75f",
   "metadata": {},
   "source": [
    "## Step 7: Check model size reduction\n",
    "\n",
    "Let's minimally quantify the benefits of quantization by comparing model sizes: the quantized model should be ~=4x smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898e225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standard (non-quantized) TFLite model for comparison\n",
    "print(\"Creating float32 TFLite model for comparison...\")\n",
    "float_converter = tf.lite.TFLiteConverter.from_keras_model(base_model)\n",
    "tflite_float_model = float_converter.convert()\n",
    "\n",
    "# Save models temporarily to measure sizes\n",
    "_, float_file = tempfile.mkstemp('.tflite')\n",
    "_, quant_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quant_file, 'wb') as f:\n",
    "    f.write(tflite_qat_model)\n",
    "\n",
    "with open(float_file, 'wb') as f:\n",
    "    f.write(tflite_float_model)\n",
    "\n",
    "# Compare sizes\n",
    "print(f\"\\nEvaluating model sizes:\")\n",
    "float_size = os.path.getsize(float_file) / (1024**2)  # MB\n",
    "quant_size = os.path.getsize(quant_file) / (1024**2)  # MB\n",
    "reduction = (1 - quant_size/float_size) * 100\n",
    "\n",
    "print(f\"\\nModel Size Comparison:\")\n",
    "print(f\"Float32 model: {float_size:.2f} MB\")\n",
    "print(f\"INT8 QAT model: {quant_size:.2f} MB\")\n",
    "print(f\"Size reduction: {reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6ae58",
   "metadata": {},
   "source": [
    "> **Why save to disk for size comparison?**: We write models to temporary files because TensorFlow's in-memory size includes additional metadata, caching, and graph structures that don't reflect true deployment size. The on-disk measurement (3.81 MB vs 13.33 MB) represents what actually gets deployed to your wildlife cameras.\n",
    "> \n",
    "> **Beyond file size metrics**: Consider analyzing:\n",
    "> - Parameter count differences (do both models have the same number of parameters?)\n",
    "> - Layer-by-layer size breakdown to identify which parts compress best\n",
    "> - Compression ratio variations across different layer types\n",
    "> - Memory footprint during inference (not just static model size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc13c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "models = ['Float32\\nTFLite', 'INT8 QAT\\nTFLite']\n",
    "sizes = [float_size, quant_size]\n",
    "colors = ['#3498db', '#2ecc71']\n",
    "\n",
    "bars = plt.bar(models, sizes, color=colors)\n",
    "plt.ylabel('Size (MB)')\n",
    "plt.title('Model Size Comparison')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, size in zip(bars, sizes):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{size:.1f} MB', ha='center', va='bottom')\n",
    "\n",
    "# Add reduction percentage at the bottom of the plot\n",
    "plt.text(0.5, -0.1, f'{reduction:.0f}% smaller', \n",
    "         ha='center', transform=plt.gca().transAxes,\n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "plt.ylim(0, max(sizes) * 1.2)  # Dynamically adjust the y-axis to fit the labels\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56443bf0",
   "metadata": {},
   "source": [
    "> **Evaluation brainstorm**: You should have confirmed the 4x size reduction, but what other evaluations would you run before deploying to wildlife cameras?\n",
    "> \n",
    "> Consider testing:\n",
    "> - Inference speed on actual NPU hardware vs CPU\n",
    "> - Accuracy on specific rare bird species images\n",
    "> - Battery consumption during continuous operation\n",
    "> - Model behavior in extreme temperatures\n",
    "> - Performance with partially occluded animals\n",
    "> - Robustness to different lighting conditions\n",
    "> \n",
    "> Which metrics matter most for your conservation goals? How would you prioritize testing given limited field deployment time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_quant_ex_revised",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you've learned how to:\n",
    "* Convert a standard model for quantization-aware training using fake quantization operations\n",
    "* Fine-tune models while simulating INT8 constraints to maintain accuracy\n",
    "* Define representative datasets for optimal TFLite conversion\n",
    "* Create INT8-optimized models ready for edge hardware acceleration\n",
    "* Evaluate quantization trade-offs on model size\n",
    "\n",
    "These skills allow you to create models specifically optimized for integer-only hardware from the training phase, rather than as an afterthought. QAT is particularly valuable when targeting specialized NPUs or when specific accuracy requirements (like rare species detection) demand the best possible INT8 performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83baac",
   "metadata": {},
   "source": [
    "> **🚀 Bonus exploration:** Why not dive deeper into the example to learn even more details about QAT? You could:\n",
    ">    - Experiment with more epochs for fine-tuning QAT, and use your actual wildlife dataset, especially images of the rare and similar-looking common birds.\n",
    ">    - Implement a full evaluation pipeline to compare the accuracy and on-device latency of the QAT TFLite model against the original, float TFLite, and PTQ TFLite models using your actual wildlife dataset and target camera hardware.\n",
    ">    - Explore different QAT configurations and schemes available in the TensorFlow Model Optimization Toolkit.\n",
    ">    - Investigate the impact of different `inference_input_type` and `inference_output_type` (e.g., `tf.uint8` vs `tf.int8`) if your hardware has specific preferences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
